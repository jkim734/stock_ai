<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.env">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.env" />
              <option name="updatedContent" value="# Google AI API Key&#10;GOOGLE_AI_API_KEY=your_google_ai_api_key_here&#10;&#10;# 사용법:&#10;# 1. Google AI Studio (https://aistudio.google.com/)에 접속&#10;# 2. API 키를 생성&#10;# 3. 위의 your_google_ai_api_key_here 부분을 실제 API 키로 교체" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/_1st_stage_news_analysis_LLM.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/_1st_stage_news_analysis_LLM.py" />
              <option name="updatedContent" value="import time&#10;from typing import Optional&#10;from dotenv import load_dotenv&#10;import os&#10;import re&#10;import json&#10;load_dotenv()&#10;&#10;from google import genai&#10;from google.genai import types&#10;&#10;model_name = &quot;gemini-2.0-flash-001&quot;&#10;&#10;# .env 파일에서 API 키 로드 (보안 강화)&#10;client = genai.Client(api_key=os.getenv(&quot;GOOGLE_AI_API_KEY&quot;))&#10;&#10;def ask_question_to_gemini_cache(prompt, max_retries=5, retry_delay=5):&#10;    &quot;&quot;&quot;&#10;    Gemini API를 사용하여 질문에 대한 답변을 얻습니다.&#10;    뉴스 분석에 최적화된 버전입니다.&#10;    &quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    for attempt in range(max_retries):&#10;        try:&#10;            # API 키 확인&#10;            api_key = os.getenv(&quot;GOOGLE_AI_API_KEY&quot;)&#10;            if not api_key:&#10;                raise Exception(&quot;GOOGLE_AI_API_KEY 환경 변수가 설정되지 않았습니다.&quot;)&#10;&#10;            api_start = time.time()&#10;&#10;            # Gemini API 호출&#10;            response = client.models.generate_content(&#10;                model=model_name,&#10;                contents=prompt,&#10;                config=types.GenerateContentConfig(&#10;                    temperature=0.3,&#10;                    max_output_tokens=2048&#10;                )&#10;            )&#10;&#10;            return response.text&#10;&#10;        except Exception as e:&#10;            error_msg = str(e).lower()&#10;            print(f&quot;API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;&#10;            if hasattr(e, 'code') and e.code == 503:&#10;                print(f&quot;⏳ API 사용량 한도 초과 (시도 {attempt + 1}/{max_retries}). {retry_delay}초 후 재시도...&quot;)&#10;                time.sleep(retry_delay)&#10;                retry_delay *= 2  # 지수적 백오프&#10;                continue&#10;&#10;            if attempt == max_retries - 1:&#10;                raise Exception(f&quot;최대 재시도 횟수 초과. 마지막 오류: {e}&quot;)&#10;&#10;            time.sleep(retry_delay)&#10;&#10;    raise Exception(&quot;모든 재시도 실패&quot;)&#10;&#10;def json_match(text):&#10;    &quot;&quot;&quot;&#10;    텍스트에서 JSON 객체를 추출하는 함수&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # 중괄호 패턴 매칭&#10;        pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'&#10;        matches = re.findall(pattern, text, re.DOTALL)&#10;        &#10;        for match in matches:&#10;            try:&#10;                return json.loads(match)&#10;            except json.JSONDecodeError:&#10;                continue&#10;        &#10;        # 백틱으로 감싸진 JSON 찾기&#10;        json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'&#10;        matches = re.findall(json_pattern, text, re.DOTALL)&#10;        &#10;        for match in matches:&#10;            try:&#10;                return json.loads(match)&#10;            except json.JSONDecodeError:&#10;                continue&#10;        &#10;        return None&#10;    except Exception as e:&#10;        print(f&quot;JSON 파싱 오류: {e}&quot;)&#10;        return None&#10;&#10;def create_news_analysis_prompt(news_text):&#10;    &quot;&quot;&quot;뉴스 분석용 프롬프트 생성&quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;다음 뉴스들을 분석하여 JSON 형태로 결과를 제공해주세요:&#10;&#10;{news_text}&#10;&#10;분석 결과를 다음 JSON 형식으로 정확히 제공해주세요:&#10;&#10;{{&#10;  &quot;overall_sentiment&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;sentiment_score&quot;: 0-100,&#10;  &quot;key_themes&quot;: [&quot;주요 테마1&quot;, &quot;주요 테마2&quot;],&#10;  &quot;market_impact&quot;: &quot;시장에 미치는 영향 분석&quot;,&#10;  &quot;summary&quot;: &quot;전체 뉴스 요약&quot;,&#10;  &quot;investment_signals&quot;: &quot;buy/sell/hold&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;&#10;def create_research_reports_analysis_prompt(reports_text):&#10;    &quot;&quot;&quot;리서치 리포트 분석용 프롬프트 생성&quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;다음 리서치 리포트들을 분석하여 JSON 형태로 결과를 제공해주세요:&#10;&#10;{reports_text}&#10;&#10;분석 결과를 다음 JSON 형식으로 정확히 제공해주세요:&#10;&#10;{{&#10;  &quot;category_summary&quot;: {{&#10;    &quot;종목분석&quot;: &quot;종목분석 요약&quot;,&#10;    &quot;산업분석&quot;: &quot;산업분석 요약&quot;,&#10;    &quot;시황정보&quot;: &quot;시황정보 요약&quot;,&#10;    &quot;투자정보&quot;: &quot;투자정보 요약&quot;&#10;  }},&#10;  &quot;top_mentioned_stocks&quot;: [&quot;종목1&quot;, &quot;종목2&quot;, &quot;종목3&quot;],&#10;  &quot;key_industries&quot;: [&quot;업종1&quot;, &quot;업종2&quot;, &quot;업종3&quot;],&#10;  &quot;investment_themes&quot;: [&quot;투자테마1&quot;, &quot;투자테마2&quot;],&#10;  &quot;market_outlook&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;risk_factors&quot;: [&quot;리스크1&quot;, &quot;리스크2&quot;],&#10;  &quot;opportunities&quot;: [&quot;기회1&quot;, &quot;기회2&quot;],&#10;  &quot;analyst_consensus&quot;: &quot;애널리스트 consensus&quot;,&#10;  &quot;summary&quot;: &quot;전체 리포트 종합 요약&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;&#10;def create_individual_news_analysis_prompt(news_article):&#10;    &quot;&quot;&quot;개별 뉴스 분석용 프롬프트 생성&quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;다음 뉴스 기사를 분석하여 JSON 형태로 결과를 제공해주세요:&#10;&#10;제목: {news_article.get('title', '')}&#10;내용: {news_article.get('content', '')[:1000]}...&#10;&#10;분석 결과를 다음 JSON 형식으로 정확히 제공해주세요:&#10;&#10;{{&#10;  &quot;sentiment&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;confidence&quot;: 0.0-1.0,&#10;  &quot;key_points&quot;: [&quot;핵심 포인트1&quot;, &quot;핵심 포인트2&quot;],&#10;  &quot;mentioned_companies&quot;: [&quot;언급된 회사1&quot;, &quot;언급된 회사2&quot;],&#10;  &quot;market_impact&quot;: &quot;high/medium/low&quot;,&#10;  &quot;investment_signal&quot;: &quot;buy/sell/hold&quot;,&#10;  &quot;summary&quot;: &quot;기사 요약&quot;&#10;}}&#10;&quot;&quot;&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/debug_crawler.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/debug_crawler.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;네이버 증권 크롤링 디버깅용 스크립트&#10;실제 페이지 구조를 확인하여 문제점을 파악합니다.&#10;&quot;&quot;&quot;&#10;&#10;import requests&#10;from bs4 import BeautifulSoup&#10;import json&#10;&#10;def debug_research_page():&#10;    &quot;&quot;&quot;메인 리서치 페이지 구조 분석&quot;&quot;&quot;&#10;    url = &quot;https://finance.naver.com/research/&quot;&#10;    headers = {&#10;        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'&#10;    }&#10;    &#10;    try:&#10;        response = requests.get(url, headers=headers)&#10;        response.raise_for_status()&#10;        &#10;        soup = BeautifulSoup(response.content, 'html.parser')&#10;        &#10;        print(&quot;=== 네이버 증권 리서치 페이지 구조 분석 ===&quot;)&#10;        &#10;        # 모든 box_type_m 찾기&#10;        sections = soup.find_all('div', {'class': 'box_type_m'})&#10;        print(f&quot;box_type_m 섹션 개수: {len(sections)}&quot;)&#10;        &#10;        for i, section in enumerate(sections):&#10;            print(f&quot;\n--- 섹션 {i} ---&quot;)&#10;            &#10;            # 제목 찾기&#10;            title_elem = section.find('h4', {'class': 'h_sub2'})&#10;            if title_elem:&#10;                print(f&quot;제목: {title_elem.get_text(strip=True)}&quot;)&#10;            &#10;            # 테이블 찾기&#10;            table = section.find('table', {'class': 'type_1'})&#10;            if table:&#10;                print(&quot;테이블 발견!&quot;)&#10;                &#10;                # 테이블 헤더 확인&#10;                headers = table.find_all('th')&#10;                if headers:&#10;                    header_texts = [th.get_text(strip=True) for th in headers]&#10;                    print(f&quot;헤더: {header_texts}&quot;)&#10;                &#10;                # 첫 번째 데이터 행 확인&#10;                tbody = table.find('tbody')&#10;                if tbody:&#10;                    rows = tbody.find_all('tr')&#10;                else:&#10;                    rows = table.find_all('tr')[1:]  # 헤더 제외&#10;                &#10;                if rows:&#10;                    first_row = rows[0]&#10;                    cells = first_row.find_all('td')&#10;                    if cells:&#10;                        cell_texts = [td.get_text(strip=True) for td in cells]&#10;                        print(f&quot;첫 번째 행 데이터: {cell_texts}&quot;)&#10;                        &#10;                        # 링크 확인&#10;                        for j, cell in enumerate(cells):&#10;                            link = cell.find('a')&#10;                            if link:&#10;                                print(f&quot;  셀 {j}에 링크 발견: {link.get('href')}&quot;)&#10;            else:&#10;                print(&quot;테이블 없음&quot;)&#10;        &#10;        # 전체 페이지에서 &quot;종목분석&quot; 텍스트 찾기&#10;        page_text = soup.get_text()&#10;        if &quot;종목분석&quot; in page_text:&#10;            print(f&quot;\n'종목분석' 텍스트 발견됨&quot;)&#10;        else:&#10;            print(f&quot;\n'종목분석' 텍스트 없음&quot;)&#10;            &#10;    except Exception as e:&#10;        print(f&quot;오류 발생: {e}&quot;)&#10;&#10;def debug_news_detail():&#10;    &quot;&quot;&quot;뉴스 상세 페이지 구조 분석&quot;&quot;&quot;&#10;    # 샘플 뉴스 링크 (상대 경로를 절대 경로로 변환)&#10;    base_url = &quot;https://finance.naver.com&quot;&#10;    sample_link = &quot;/news/news_read.naver?article_id=0001218141&amp;office_id=215&amp;mode=mainnews&amp;type=&amp;date=2025-07-29&amp;page=1&quot;&#10;    full_url = base_url + sample_link&#10;    &#10;    headers = {&#10;        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'&#10;    }&#10;    &#10;    try:&#10;        response = requests.get(full_url, headers=headers)&#10;        response.raise_for_status()&#10;        &#10;        soup = BeautifulSoup(response.content, 'html.parser')&#10;        &#10;        print(&quot;\n=== 뉴스 상세 페이지 구조 분석 ===&quot;)&#10;        print(f&quot;URL: {full_url}&quot;)&#10;        &#10;        # 다양한 선택자로 본문 찾기 시도&#10;        content_selectors = [&#10;            '.newsct_article',&#10;            '.articleCont', &#10;            '.news_content',&#10;            '.article_view',&#10;            '.view_text',&#10;            '#news_read'&#10;        ]&#10;        &#10;        for selector in content_selectors:&#10;            element = soup.select_one(selector)&#10;            if element:&#10;                content = element.get_text(strip=True)&#10;                print(f&quot;선택자 '{selector}' 성공 - 내용 길이: {len(content)}&quot;)&#10;                if len(content) &gt; 0:&#10;                    print(f&quot;내용 미리보기: {content[:100]}...&quot;)&#10;                    break&#10;        else:&#10;            print(&quot;본문을 찾을 수 없음&quot;)&#10;        &#10;        # 날짜 찾기 시도&#10;        date_selectors = [&#10;            '.article_info .date',&#10;            '.newsct_date',&#10;            '.news_date',&#10;            '.date_time'&#10;        ]&#10;        &#10;        for selector in date_selectors:&#10;            element = soup.select_one(selector)&#10;            if element:&#10;                date_text = element.get_text(strip=True)&#10;                print(f&quot;선택자 '{selector}' 성공 - 날짜: {date_text}&quot;)&#10;                break&#10;        else:&#10;            print(&quot;날짜를 찾을 수 없음&quot;)&#10;            &#10;    except Exception as e:&#10;        print(f&quot;뉴스 상세 페이지 오류: {e}&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    debug_research_page()&#10;    debug_news_detail()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/diagnose_gemini.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/diagnose_gemini.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Gemini API 연결 진단 스크립트&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;from dotenv import load_dotenv&#10;import google.generativeai as genai&#10;&#10;def diagnose_gemini_connection():&#10;    &quot;&quot;&quot;Gemini API 연결 상태를 진단합니다.&quot;&quot;&quot;&#10;    &#10;    print(&quot; Gemini API 연결 진단 시작...&quot;)&#10;    &#10;    # 1. 환경 변수 확인&#10;    load_dotenv()&#10;    api_key = os.getenv(&quot;GOOGLE_AI_API_KEY&quot;)&#10;    &#10;    if not api_key:&#10;        print(&quot;❌ GOOGLE_AI_API_KEY 환경 변수가 설정되지 않았습니다.&quot;)&#10;        print(&quot; 해결 방법:&quot;)&#10;        print(&quot;   1. .env 파일을 생성하고 다음 내용을 추가하세요:&quot;)&#10;        print(&quot;      GOOGLE_AI_API_KEY=your_actual_api_key_here&quot;)&#10;        print(&quot;   2. Google AI Studio에서 API 키를 발급받으세요: https://aistudio.google.com/app/apikey&quot;)&#10;        return False&#10;    &#10;    print(f&quot;✅ API 키 확인됨 (길이: {len(api_key)} 문자)&quot;)&#10;    &#10;    # 2. API 키 형식 확인&#10;    if not api_key.startswith('AIza'):&#10;        print(&quot;⚠️  API 키 형식이 올바르지 않을 수 있습니다. (AIza로 시작해야 함)&quot;)&#10;    &#10;    # 3. API 연결 테스트&#10;    try:&#10;        print(&quot; Gemini API 연결 테스트 중...&quot;)&#10;        genai.configure(api_key=api_key)&#10;        &#10;        # 간단한 모델 호출 테스트&#10;        model = genai.GenerativeModel(&quot;gemini-2.0-flash-001&quot;)&#10;        &#10;        print(&quot; 간단한 테스트 요청 전송 중...&quot;)&#10;        response = model.generate_content(&#10;            &quot;안녕하세요를 영어로 번역해주세요.&quot;,&#10;            generation_config=genai.types.GenerationConfig(&#10;                temperature=0.1,&#10;                max_output_tokens=50&#10;            )&#10;        )&#10;        &#10;        print(&quot;✅ API 연결 성공!&quot;)&#10;        print(f&quot; 응답: {response.text}&quot;)&#10;        &#10;        # 토큰 사용량 확인&#10;        if hasattr(response, 'usage_metadata'):&#10;            print(f&quot; 토큰 사용량: {response.usage_metadata.total_token_count}&quot;)&#10;        &#10;        return True&#10;        &#10;    except Exception as e:&#10;        print(f&quot;❌ API 연결 실패: {e}&quot;)&#10;        &#10;        # 구체적인 오류 분석&#10;        error_str = str(e).lower()&#10;        if &quot;invalid api key&quot; in error_str or &quot;authentication&quot; in error_str:&#10;            print(&quot; API 키가 유효하지 않습니다.&quot;)&#10;            print(&quot;   - Google AI Studio에서 새로운 API 키를 발급받아보세요.&quot;)&#10;            print(&quot;   - API 키를 복사할 때 공백이나 특수문자가 포함되지 않았는지 확인하세요.&quot;)&#10;        elif &quot;quota&quot; in error_str or &quot;limit&quot; in error_str:&#10;            print(&quot; API 사용량 한도 초과입니다.&quot;)&#10;            print(&quot;   - 잠시 후 다시 시도하거나 유료 플랜을 고려해보세요.&quot;)&#10;        elif &quot;network&quot; in error_str or &quot;connection&quot; in error_str:&#10;            print(&quot; 네트워크 연결 문제입니다.&quot;)&#10;            print(&quot;   - 인터넷 연결을 확인하세요.&quot;)&#10;            print(&quot;   - VPN을 사용 중이라면 끄고 시도해보세요.&quot;)&#10;        else:&#10;            print(&quot;❓ 알 수 없는 오류입니다.&quot;)&#10;            print(f&quot;   상세 오류: {e}&quot;)&#10;        &#10;        return False&#10;&#10;def create_env_template():&#10;    &quot;&quot;&quot;환경 변수 템플릿 파일을 생성합니다.&quot;&quot;&quot;&#10;    env_content = &quot;&quot;&quot;# Google Gemini API 키&#10;# https://aistudio.google.com/app/apikey 에서 발급받으세요&#10;GOOGLE_AI_API_KEY=your_api_key_here&#10;&#10;# 사용 예시:&#10;# GOOGLE_AI_API_KEY=AIzaSyBxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#10;&quot;&quot;&quot;&#10;    &#10;    with open('.env.template', 'w', encoding='utf-8') as f:&#10;        f.write(env_content)&#10;    &#10;    print(&quot; .env.template 파일이 생성되었습니다.&quot;)&#10;    print(&quot;   이 파일을 참고하여 .env 파일을 만들어주세요.&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    print(&quot;=&quot; * 50)&#10;    if not diagnose_gemini_connection():&#10;        print(&quot;\n 문제 해결 가이드:&quot;)&#10;        print(&quot;1. Google AI Studio 접속: https://aistudio.google.com/app/apikey&quot;)&#10;        print(&quot;2. 'Create API key' 클릭&quot;)&#10;        print(&quot;3. API 키 복사&quot;)&#10;        print(&quot;4. .env 파일에 GOOGLE_AI_API_KEY=복사한_키 추가&quot;)&#10;        print(&quot;5. 다시 실행&quot;)&#10;        &#10;        create_env_template()&#10;    print(&quot;=&quot; * 50)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/gemini.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/gemini.py" />
              <option name="originalContent" value="import time&#10;from typing import Optional&#10;from dotenv import load_dotenv&#10;import os&#10;import re&#10;import regex&#10;import json&#10;load_dotenv()&#10;&#10;from google import genai&#10;from google.genai import types&#10;&#10;model_name = &quot;gemini-2.0-flash-001&quot;&#10;&#10;# .env 파일에서 API 키 로드 (보안 강화)&#10;client = genai.Client(api_key=os.getenv(&quot;GOOGLE_AI_API_KEY&quot;))&#10;&#10;def ask_question_to_gemini_cache(prompt, max_retries=5, retry_delay=5):&#10;    &quot;&quot;&quot;&#10;    Gemini API를 사용하여 질문에 대한 답변을 얻습니다.&#10;    뉴스 분석에 최적화된 버전입니다.&#10;    &quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    for attempt in range(max_retries):&#10;        try:&#10;            # API 키 확인&#10;            api_key = os.getenv(&quot;GOOGLE_AI_API_KEY&quot;)&#10;            if not api_key:&#10;                raise Exception(&quot;GOOGLE_AI_API_KEY 환경 변수가 설정되지 않았습니다.&quot;)&#10;&#10;            api_start = time.time()&#10;&#10;            # Gemini API 호출&#10;            response = client.models.generate_content(&#10;                model=model_name,&#10;                contents=prompt,&#10;                config=types.GenerateContentConfig(&#10;                    temperature=0.3,&#10;                    max_output_tokens=2048&#10;                )&#10;            )&#10;&#10;            return response.text&#10;&#10;        except Exception as e:&#10;            error_msg = str(e).lower()&#10;            print(f&quot;API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;&#10;            if hasattr(e, 'code') and e.code == 503:&#10;                print(f&quot;⏳ API 사용량 한도 초과 (시도 {attempt + 1}/{max_retries}). {retry_delay}초 후 재시도...&quot;)&#10;                time.sleep(retry_delay)&#10;            elif &quot;invalid api key&quot; in error_msg or &quot;authentication&quot; in error_msg:&#10;                print(&quot;❌ API 키가 유효하지 않습니다. .env 파일을 확인하세요.&quot;)&#10;                break&#10;            elif &quot;quota&quot; in error_msg or &quot;limit&quot; in error_msg:&#10;                print(f&quot;⏳ API 사용량 한도 초과. {retry_delay}초 후 재시도...&quot;)&#10;                time.sleep(retry_delay)&#10;            else:&#10;                print(f&quot; Gemini API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;                if attempt == max_retries - 1:&#10;                    raise&#10;&#10;    print(f&quot;{max_retries}번 시도 후 실패. 총 소요시간: {time.time() - start_time:.2f}초&quot;)&#10;    return None&#10;&#10;def create_news_analysis_prompt(combined_text: str) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    뉴스 분석을 위한 통합 프롬프트 생성&#10;    &quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;너는 국내 상장사와 주요 산업 분석에 특화된 금융 뉴스 분석 전문가야.&#10;&#10;{combined_text}&#10;&#10;뉴스 기사에 대해, 기계적 추출이 아닌 **생성적 요약**으로 다음 항목들을 JSON 형식으로 분석해주세요:&#10;&#10;1. overall_sentiment: 전체적인 시장 감정 (positive/negative/neutral)&#10;2. sentiment_score: 감정 점수 (-100 ~ +100, 숫자로만)&#10;3. key_themes: 주요 테마들 (배열)&#10;4. market_impact: 시장에 미칠 영향 예측 (2-3문장)&#10;5. summary: 전체 뉴스 요약 (3-4문장)&#10;6. investment_signals: 투자 시그널 (buy/sell/hold)&#10;&#10;### 출력 형식 (JSON)&#10;{{&#10;  &quot;overall_sentiment&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;sentiment_score&quot;: 숫자,&#10;  &quot;key_themes&quot;: [&quot;테마1&quot;, &quot;테마2&quot;, &quot;테마3&quot;],&#10;  &quot;market_impact&quot;: &quot;시장 영향 분석&quot;,&#10;  &quot;summary&quot;: &quot;전체 뉴스 요약&quot;,&#10;  &quot;investment_signals&quot;: &quot;buy/sell/hold&quot;&#10;}}&#10;&#10;### 분석 가이드라인:&#10;• 답변 형식: 반드시 유효한 JSON 구조로 작성해주세요&#10;• 언어: 한국어로 자연스럽게 작성&#10;• 분석 스타일: 객관적이고 균형잡힌 관점 유지&#10;• 근거: 실제 데이터와 시장 동향에 기반한 분석&#10;• 투자 조언: 과도한 투기보다는 신중한 투자 관점 제시&#10;&#10;응답은 반드시 JSON 형식으로만 해주세요.&#10;&quot;&quot;&quot;&#10;&#10;def ask_news_analysis(prompt, max_retries=3):&#10;    &quot;&quot;&quot;&#10;    뉴스 분석 전용 Gemini API 호출 함수&#10;    JSON 응답을 기대하는 뉴스 분석에 최적화&#10;    &quot;&quot;&quot;&#10;    return ask_question_to_gemini_cache(prompt, max_retries=max_retries)&#10;&#10;def create_research_reports_analysis_prompt(combined_text: str) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    리서치 리포트 분석을 위한 통합 프롬프트 생성&#10;    &quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;너는 국내 증권사 리서치 분석에 특화된 금융 전문가야.&#10;&#10;다음 네이버 증권 리서치 뉴스와 리포트들을 카테고리별로 분석해주세요:&#10;&#10;{combined_text}&#10;&#10;뉴스와 리포트에 대해, 종합적이고 심층적인 분석으로 다음 항목들을 JSON 형식으로 분석해주세요:&#10;&#10;1. category_summary: 카테고리별 요약 (각 카테고리당 2-3문장씩, 객체 형태)&#10;2. top_mentioned_stocks: 가장 많이 언급된 종목들 (배열, 최대 10개)&#10;3. key_industries: 주요 관심 산업/섹터들 (배열, 최대 8개)&#10;4. investment_themes: 주요 투자 테마들 (배열, 최대 8개)&#10;5. market_outlook: 전체 시장 전망 (bullish/bearish/neutral)&#10;6. risk_factors: 주요 리스크 요인들 (배열, 각 한 문장, 최대 5개)&#10;7. opportunities: 투자 기회들 (배열, 각 한 문장, 최대 5개)&#10;8. analyst_consensus: 애널리스트들의 전반적 합의 사항 (2-3문장)&#10;9. summary: 전체 리포트 종합 분석 (4-5문장)&#10;&#10;### 출력 형식 (JSON)&#10;{{&#10;  &quot;category_summary&quot;: {{&#10;    &quot;종목분석&quot;: &quot;종목분석 카테고리 요약 2-3문장&quot;,&#10;    &quot;산업분석&quot;: &quot;산업분석 카테고리 요약 2-3문장&quot;,&#10;    &quot;시황정보&quot;: &quot;시황정보 카테고리 요약 2-3문장&quot;,&#10;    &quot;투자정보&quot;: &quot;투자정보 카테고리 요약 2-3문장&quot;&#10;  }},&#10;  &quot;top_mentioned_stocks&quot;: [&quot;종목1&quot;, &quot;종목2&quot;, &quot;종목3&quot;],&#10;  &quot;key_industries&quot;: [&quot;산업1&quot;, &quot;산업2&quot;, &quot;산업3&quot;],&#10;  &quot;investment_themes&quot;: [&quot;테마1&quot;, &quot;테마2&quot;, &quot;테마3&quot;],&#10;  &quot;market_outlook&quot;: &quot;bullish/bearish/neutral&quot;,&#10;  &quot;risk_factors&quot;: [&quot;리스크 요인 1&quot;, &quot;리스크 요인 2&quot;],&#10;  &quot;opportunities&quot;: [&quot;투자 기회 1&quot;, &quot;투자 기회 2&quot;],&#10;  &quot;analyst_consensus&quot;: &quot;애널리스트 합의 사항 2-3문장&quot;,&#10;  &quot;summary&quot;: &quot;전체 종합 분석 4-5문장&quot;&#10;}}&#10;&#10;### 분석 가이드라인:&#10;• 답변 형식: 반드시 유효한 JSON 구조로 작성해주세요&#10;• 언어: 한국어로 자연스럽게 작성&#10;• 분석 스타일: 객관적이고 전문적인 관점 유지&#10;• 근거: 실제 리포트 내용과 시장 동향에 기반한 분석&#10;• 투자 조언: 신중하고 균형잡힌 투자 관점 제시&#10;• 완성도: 모든 9개 항목을 빠짐없이 포함해야 함&#10;&#10;응답은 반드시 JSON 형식으로만 해주세요.&#10;&quot;&quot;&quot;&#10;&#10;def json_match(input_string):&#10;    &quot;&quot;&quot;&#10;    Use regex to extract JSON from a string.&#10;    &quot;&quot;&quot;&#10;    if not input_string:&#10;        return None&#10;&#10;    print(&quot;응답 텍스트에서 JSON 추출 시도...&quot;)&#10;&#10;    # 1. 백틱으로 감싸진 JSON 찾기 (개선된 regex 사용)&#10;    pattern_backticks = r'```json\s*(\{.*?\})\s*```'&#10;    m = re.search(pattern_backticks, input_string, re.DOTALL)&#10;    if m:&#10;        json_str = m.group(1)&#10;        try:&#10;            result = json.loads(json_str)&#10;            print(&quot;백틱 JSON 추출 성공&quot;)&#10;            return result&#10;        except json.JSONDecodeError as e:&#10;            print(f&quot;백틱 JSON 파싱 실패: {e}&quot;)&#10;            # 손상된 JSON 복구 시도&#10;            fixed_json = _try_fix_json(json_str)&#10;            if fixed_json:&#10;                return fixed_json&#10;&#10;    # 2. regex 라이브러리가 있다면 고급 패턴 사용&#10;    try:&#10;        pattern_simple = r'(\{(?:[^{}]|(?R))*\})'&#10;        m = regex.search(pattern_simple, input_string)&#10;        if m:&#10;            json_str = m.group(1)&#10;            try:&#10;                result = json.loads(json_str)&#10;                print(&quot;고급 regex JSON 추출 성공&quot;)&#10;                return result&#10;            except json.JSONDecodeError as e:&#10;                print(f&quot;고급 regex JSON 파싱 실패: {e}&quot;)&#10;                # 손상된 JSON 복구 시도&#10;                fixed_json = _try_fix_json(json_str)&#10;                if fixed_json:&#10;                    return fixed_json&#10;    except NameError:&#10;        # regex 라이브러리가 없는 경우 기본 패턴 사용&#10;        pass&#10;&#10;    # 3. 기본 re 모듈로 간단한 패턴 시도&#10;    try:&#10;        # 여러 JSON 객체를 찾아서 가장 완전한 것 선택&#10;        json_candidates = []&#10;&#10;        # 모든 { } 찾기&#10;        brace_count = 0&#10;        start_pos = -1&#10;&#10;        for i, char in enumerate(input_string):&#10;            if char == '{':&#10;                if brace_count == 0:&#10;                    start_pos = i&#10;                brace_count += 1&#10;            elif char == '}':&#10;                brace_count -= 1&#10;                if brace_count == 0 and start_pos != -1:&#10;                    json_candidate = input_string[start_pos:i+1]&#10;                    json_candidates.append(json_candidate)&#10;&#10;        # 가장 긴 JSON 후보를 먼저 시도&#10;        json_candidates.sort(key=len, reverse=True)&#10;&#10;        for json_str in json_candidates:&#10;            try:&#10;                result = json.loads(json_str)&#10;                print(&quot;기본 JSON 추출 성공&quot;)&#10;                return result&#10;            except json.JSONDecodeError:&#10;                # 손상된 JSON 복구 시도&#10;                fixed_json = _try_fix_json(json_str)&#10;                if fixed_json:&#10;                    return fixed_json&#10;                continue&#10;&#10;    except Exception as e:&#10;        print(f&quot;기본 JSON 파싱 중 오류: {e}&quot;)&#10;&#10;    print(&quot;JSON 추출 실패&quot;)&#10;    return None&#10;&#10;def _try_fix_json(json_str: str) -&gt; Optional[dict]:&#10;    &quot;&quot;&quot;&#10;    손상된 JSON을 복구하려고 시도&#10;    &quot;&quot;&quot;&#10;    print(&quot;손상된 JSON 복구 시도...&quot;)&#10;&#10;    try:&#10;        # 1. 흔한 문제들 수정&#10;        fixed_str = json_str&#10;&#10;        # 잘못된 키 이름 패턴 수정 (예: &quot;텍스트key&quot;: -&gt; &quot;key&quot;:)&#10;        import re&#10;        fixed_str = re.sub(r'&quot;[^&quot;]*[가-힣][^&quot;]*([a-zA-Z_][a-zA-Z0-9_]*)&quot;:', r'&quot;\1&quot;:', fixed_str)&#10;&#10;        # 중간에 끊어진 문자열 + 키 패턴 수정&#10;        fixed_str = re.sub(r'&quot;[^&quot;]*&quot;([a-zA-Z_][a-zA-Z0-9_]*)&quot;:', r'&quot;, &quot;\1&quot;:', fixed_str)&#10;&#10;        # 잘 구분된 쉼표 패턴 수정&#10;        fixed_str = re.sub(r',\s*}', '}', fixed_str)&#10;        fixed_str = re.sub(r',\s*]', ']', fixed_str)&#10;&#10;        # 2. JSON 파싱 재시도&#10;        result = json.loads(fixed_str)&#10;        print(&quot;JSON 복구 성공!&quot;)&#10;        return result&#10;&#10;    except json.JSONDecodeError as e:&#10;        print(f&quot;JSON 복구 실패: {e}&quot;)&#10;&#10;        # 3. 부분 복구 시도 - 유효한 필드만 추출&#10;        try:&#10;            partial_data = {}&#10;&#10;            # 간단한 키-값 쌍 추출&#10;            simple_patterns = [&#10;                (r'&quot;([^&quot;]+)&quot;:\s*&quot;([^&quot;]*)&quot;', str),  # 문자열 값&#10;                (r'&quot;([^&quot;]+)&quot;:\s*(\d+(?:\.\d+)?)', float),  # 숫자 값&#10;                (r'&quot;([^&quot;]+)&quot;:\s*(true|false)', bool),  # 불린 값&#10;            ]&#10;&#10;            for pattern, value_type in simple_patterns:&#10;                matches = re.findall(pattern, json_str)&#10;                for key, value in matches:&#10;                    try:&#10;                        if value_type == bool:&#10;                            partial_data[key] = value.lower() == 'true'&#10;                        elif value_type == float:&#10;                            partial_data[key] = float(value)&#10;                        else:&#10;                            partial_data[key] = value&#10;                    except:&#10;                        continue&#10;&#10;            if partial_data:&#10;                print(f&quot;부분 JSON 복구 성공: {len(partial_data)}개 필드&quot;)&#10;                return partial_data&#10;&#10;        except Exception as e:&#10;            print(f&quot;부분 복구도 실패: {e}&quot;)&#10;&#10;    return None" />
              <option name="updatedContent" value="import time&#10;from typing import Optional&#10;from dotenv import load_dotenv&#10;import os&#10;import re&#10;import regex&#10;import json&#10;load_dotenv()&#10;&#10;from google import genai&#10;from google.genai import types&#10;&#10;model_name = &quot;gemini-2.0-flash-001&quot;&#10;&#10;# .env 파일에서 API 키 로드 (보안 강화)&#10;client = genai.Client(api_key=os.getenv(&quot;GOOGLE_AI_API_KEY&quot;))&#10;&#10;def ask_question_to_gemini_cache(prompt, max_retries=5, retry_delay=5):&#10;    &quot;&quot;&quot;&#10;    Gemini API를 사용하여 질문에 대한 답변을 얻습니다.&#10;    뉴스 분석에 최적화된 버전입니다.&#10;    &quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    for attempt in range(max_retries):&#10;        try:&#10;            # API 키 확인&#10;            api_key = os.getenv(&quot;GOOGLE_AI_API_KEY&quot;)&#10;            if not api_key:&#10;                raise Exception(&quot;GOOGLE_AI_API_KEY 환경 변수가 설정되지 않았습니다.&quot;)&#10;&#10;            api_start = time.time()&#10;&#10;            # Gemini API 호출&#10;            response = client.models.generate_content(&#10;                model=model_name,&#10;                contents=prompt,&#10;                config=types.GenerateContentConfig(&#10;                    temperature=0.3,&#10;                    max_output_tokens=2048&#10;                )&#10;            )&#10;&#10;            return response.text&#10;&#10;        except Exception as e:&#10;            error_msg = str(e).lower()&#10;            print(f&quot;API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;&#10;            if hasattr(e, 'code') and e.code == 503:&#10;                print(f&quot;⏳ API 사용량 한도 초과 (시도 {attempt + 1}/{max_retries}). {retry_delay}초 후 재시도...&quot;)&#10;                time.sleep(retry_delay)&#10;            elif &quot;invalid api key&quot; in error_msg or &quot;authentication&quot; in error_msg:&#10;                print(&quot;❌ API 키가 유효하지 않습니다. .env 파일을 확인하세요.&quot;)&#10;                break&#10;            elif &quot;quota&quot; in error_msg or &quot;limit&quot; in error_msg:&#10;                print(f&quot;⏳ API 사용량 한도 초과. {retry_delay}초 후 재시도...&quot;)&#10;                time.sleep(retry_delay)&#10;            else:&#10;                print(f&quot; Gemini API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;                if attempt == max_retries - 1:&#10;                    raise&#10;&#10;    print(f&quot;{max_retries}번 시도 후 실패. 총 소요시간: {time.time() - start_time:.2f}초&quot;)&#10;    return None&#10;&#10;def create_news_analysis_prompt(combined_text: str) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    뉴스 분석을 위한 통합 프롬프트 생성&#10;    &quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;너는 국내 상장사와 주요 산업 분석에 특화된 금융 뉴스 분석 전문가야.&#10;&#10;{combined_text}&#10;&#10;뉴스 기사에 대해, 기계적 추출이 아닌 **생성적 요약**으로 다음 항목들을 JSON 형식으로 분석해주세요:&#10;&#10;1. overall_sentiment: 전체적인 시장 감정 (positive/negative/neutral)&#10;2. sentiment_score: 감정 점수 (-100 ~ +100, 숫자로만)&#10;3. key_themes: 주요 테마들 (배열)&#10;4. market_impact: 시장에 미칠 영향 예측 (2-3문장)&#10;5. summary: 전체 뉴스 요약 (3-4문장)&#10;6. investment_signals: 투자 시그널 (buy/sell/hold)&#10;&#10;### 출력 형식 (JSON)&#10;{{&#10;  &quot;overall_sentiment&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;sentiment_score&quot;: 숫자,&#10;  &quot;key_themes&quot;: [&quot;테마1&quot;, &quot;테마2&quot;, &quot;테마3&quot;],&#10;  &quot;market_impact&quot;: &quot;시장 영향 분석&quot;,&#10;  &quot;summary&quot;: &quot;전체 뉴스 요약&quot;,&#10;  &quot;investment_signals&quot;: &quot;buy/sell/hold&quot;&#10;}}&#10;&#10;### 분석 가이드라인:&#10;• 답변 형식: 반드시 유효한 JSON 구조로 작성해주세요&#10;• 언어: 한국어로 자연스럽게 작성&#10;• 분석 스타일: 객관적이고 균형잡힌 관점 유지&#10;• 근거: 실제 데이터와 시장 동향에 기반한 분석&#10;• 투자 조언: 과도한 투기보다는 신중한 투자 관점 제시&#10;&#10;응답은 반드시 JSON 형식으로만 해주세요.&#10;&quot;&quot;&quot;&#10;&#10;def ask_news_analysis(prompt, max_retries=3):&#10;    &quot;&quot;&quot;&#10;    뉴스 분석 전용 Gemini API 호출 함수&#10;    JSON 응답을 기대하는 뉴스 분석에 최적화&#10;    &quot;&quot;&quot;&#10;    return ask_question_to_gemini_cache(prompt, max_retries=max_retries)&#10;&#10;def create_research_reports_analysis_prompt(combined_text: str) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    리서치 리포트 분석을 위한 통합 프롬프트 생성&#10;    &quot;&quot;&quot;&#10;    return f&quot;&quot;&quot;&#10;너는 국내 증권사 리서치 분석에 특화된 금융 전문가야.&#10;&#10;다음 네이버 증권 리서치 뉴스와 리포트들을 카테고리별로 분석해주세요:&#10;&#10;{combined_text}&#10;&#10;뉴스와 리포트에 대해, 종합적이고 심층적인 분석으로 다음 항목들을 JSON 형식으로 분석해주세요:&#10;&#10;1. category_summary: 카테고리별 요약 (각 카테고리당 2-3문장씩, 객체 형태)&#10;2. top_mentioned_stocks: 가장 많이 언급된 종목들 (배열, 최대 10개)&#10;3. key_industries: 주요 관심 산업/섹터들 (배열, 최대 8개)&#10;4. investment_themes: 주요 투자 테마들 (배열, 최대 8개)&#10;5. market_outlook: 전체 시장 전망 (bullish/bearish/neutral)&#10;6. risk_factors: 주요 리스크 요인들 (배열, 각 한 문장, 최대 5개)&#10;7. opportunities: 투자 기회들 (배열, 각 한 문장, 최대 5개)&#10;8. analyst_consensus: 애널리스트들의 전반적 합의 사항 (2-3문장)&#10;9. summary: 전체 리포트 종합 분석 (4-5문장)&#10;&#10;### 출력 형식 (JSON)&#10;{{&#10;  &quot;category_summary&quot;: {{&#10;    &quot;종목분석&quot;: &quot;종목분석 카테고리 요약 2-3문장&quot;,&#10;    &quot;산업분석&quot;: &quot;산업분석 카테고리 요약 2-3문장&quot;,&#10;    &quot;시황정보&quot;: &quot;시황정보 카테고리 요약 2-3문장&quot;,&#10;    &quot;투자정보&quot;: &quot;투자정보 카테고리 요약 2-3문장&quot;&#10;  }},&#10;  &quot;top_mentioned_stocks&quot;: [&quot;종목1&quot;, &quot;종목2&quot;, &quot;종목3&quot;],&#10;  &quot;key_industries&quot;: [&quot;산업1&quot;, &quot;산업2&quot;, &quot;산업3&quot;],&#10;  &quot;investment_themes&quot;: [&quot;테마1&quot;, &quot;테마2&quot;, &quot;테마3&quot;],&#10;  &quot;market_outlook&quot;: &quot;bullish/bearish/neutral&quot;,&#10;  &quot;risk_factors&quot;: [&quot;리스크 요인 1&quot;, &quot;리스크 요인 2&quot;],&#10;  &quot;opportunities&quot;: [&quot;투자 기회 1&quot;, &quot;투자 기회 2&quot;],&#10;  &quot;analyst_consensus&quot;: &quot;애널리스트 합의 사항 2-3문장&quot;,&#10;  &quot;summary&quot;: &quot;전체 종합 분석 4-5문장&quot;&#10;}}&#10;&#10;### 분석 가이드라인:&#10;• 답변 형식: 반드시 유효한 JSON 구조로 작성해주세요&#10;• 언어: 한국어로 자연스럽게 작성&#10;• 분석 스타일: 객관적이고 전문적인 관점 유지&#10;• 근거: 실제 리포트 내용과 시장 동향에 기반한 분석&#10;• 투자 조언: 신중하고 균형잡힌 투자 관점 제시&#10;• 완성도: 모든 9개 항목을 빠짐없이 포함해야 함&#10;&#10;응답은 반드시 JSON 형식으로만 해주세요.&#10;&quot;&quot;&quot;&#10;&#10;def create_individual_news_analysis_prompt(news_item: dict) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    개별 뉴스 분석을 위한 프롬프트 생성&#10;    &quot;&quot;&quot;&#10;    title = news_item.get('title', '')&#10;    content = news_item.get('content', '')&#10;    &#10;    # 내용이 없으면 제목만 사용&#10;    text_to_analyze = content if content.strip() else title&#10;    &#10;    return f&quot;&quot;&quot;&#10;너는 국내 상장사와 주요 산업 분석에 특화된 금융 뉴스 분석 전문가야.&#10;&#10;다음 뉴스 기사를 분석해주세요:&#10;&#10;제목: {title}&#10;내용: {text_to_analyze[:500]}&#10;&#10;뉴스 기사에 대해, 기계적 추출이 아닌 **생성적 요약**으로 다음 항목들을 JSON 형식으로 분석해주세요:&#10;&#10;1. summary: 주요 이슈 한 문장 요약&#10;2. entities: 관련 기업명과/또는 업종명 명시&#10;3. impact: 업계·시장 파급 영��� 또는 의미 부각&#10;4. type: 기업 또는 산업 (정확히 이 두 단어 중 하나만)&#10;5. reason: 이 기사 분류의 근거 (기업/산업)&#10;&#10;### 분류 지침:&#10;- 특정 기업(예: 삼성전자, 현대차 등)에 대한 기사면 &quot;type&quot;: &quot;기업&quot;으로,&#10;- 정부 정책, 산업 정책, 경제 지표(금리, 환율 등), 법/제도, 또는 특정 산업 전체에 대한 기사면 &quot;type&quot;: &quot;산업&quot;으로 분류.&#10;- 분류 사유는 기사 내용 근거를 1문장으로 명확히 설명할 것.&#10;&#10;### 출력 형식 (JSON)&#10;{{&#10;  &quot;summary&quot;: &quot;&lt;주요 이슈 한 문장 요약&gt;&quot;,&#10;  &quot;entities&quot;: &quot;&lt;관련 기업명과/또는 업종명 명시&gt;&quot;,&#10;  &quot;impact&quot;: &quot;&lt;업계·시장 파급 영향 또는 의미 부각&gt;&quot;,&#10;  &quot;type&quot;: &quot;기업 또는 산업&quot;,&#10;  &quot;reason&quot;: &quot;&lt;이 기사 분류의 근거 (기업/산업)&gt;&quot;&#10;}}&#10;&#10;### 분석 가이드라인:&#10;• 답변 형식: 반드시 유효한 JSON 구조로 작성해주세요&#10;• 언어: 한국어로 자연스럽게 작��&#10;• 분석 스타일: 객관적이고 균형잡힌 관점 유지&#10;• 근거: 실제 데이터와 시장 동향에 기반한 분석&#10;• 투자 조언: 과도한 투기보다는 신중한 투자 관점 제시&#10;&#10;응답은 반드시 JSON 형식으로만 해주세요.&#10;&quot;&quot;&quot;&#10;&#10;def json_match(input_string):&#10;    &quot;&quot;&quot;&#10;    Use regex to extract JSON from a string.&#10;    &quot;&quot;&quot;&#10;    if not input_string:&#10;        return None&#10;&#10;    print(&quot;응답 텍스트에서 JSON 추출 시도...&quot;)&#10;&#10;    # 1. 백틱으로 감싸진 JSON 찾기 (개선된 regex 사용)&#10;    pattern_backticks = r'```json\s*(\{.*?\})\s*```'&#10;    m = re.search(pattern_backticks, input_string, re.DOTALL)&#10;    if m:&#10;        json_str = m.group(1)&#10;        try:&#10;            result = json.loads(json_str)&#10;            print(&quot;백틱 JSON 추출 성공&quot;)&#10;            return result&#10;        except json.JSONDecodeError as e:&#10;            print(f&quot;백틱 JSON 파싱 실패: {e}&quot;)&#10;            # 손상된 JSON 복구 시도&#10;            fixed_json = _try_fix_json(json_str)&#10;            if fixed_json:&#10;                return fixed_json&#10;&#10;    # 2. regex 라이브러리가 있다면 고급 패턴 사용&#10;    try:&#10;        pattern_simple = r'(\{(?:[^{}]|(?R))*\})'&#10;        m = regex.search(pattern_simple, input_string)&#10;        if m:&#10;            json_str = m.group(1)&#10;            try:&#10;                result = json.loads(json_str)&#10;                print(&quot;고급 regex JSON 추출 성공&quot;)&#10;                return result&#10;            except json.JSONDecodeError as e:&#10;                print(f&quot;고급 regex JSON 파싱 실패: {e}&quot;)&#10;                # 손상된 JSON 복구 시도&#10;                fixed_json = _try_fix_json(json_str)&#10;                if fixed_json:&#10;                    return fixed_json&#10;    except NameError:&#10;        # regex 라이브러리가 없는 경우 기본 패턴 사용&#10;        pass&#10;&#10;    # 3. 기본 re 모듈로 간단한 패턴 시도&#10;    try:&#10;        # 여러 JSON 객체를 찾아서 가장 완전한 것 선택&#10;        json_candidates = []&#10;&#10;        # 모든 { } 찾기&#10;        brace_count = 0&#10;        start_pos = -1&#10;&#10;        for i, char in enumerate(input_string):&#10;            if char == '{':&#10;                if brace_count == 0:&#10;                    start_pos = i&#10;                brace_count += 1&#10;            elif char == '}':&#10;                brace_count -= 1&#10;                if brace_count == 0 and start_pos != -1:&#10;                    json_candidate = input_string[start_pos:i+1]&#10;                    json_candidates.append(json_candidate)&#10;&#10;        # 가장 긴 JSON 후보를 먼저 시도&#10;        json_candidates.sort(key=len, reverse=True)&#10;&#10;        for json_str in json_candidates:&#10;            try:&#10;                result = json.loads(json_str)&#10;                print(&quot;기본 JSON 추출 성공&quot;)&#10;                return result&#10;            except json.JSONDecodeError:&#10;                # 손상된 JSON 복구 시도&#10;                fixed_json = _try_fix_json(json_str)&#10;                if fixed_json:&#10;                    return fixed_json&#10;                continue&#10;&#10;    except Exception as e:&#10;        print(f&quot;기본 JSON 파싱 중 오류: {e}&quot;)&#10;&#10;    print(&quot;JSON 추출 실패&quot;)&#10;    return None&#10;&#10;def _try_fix_json(json_str: str) -&gt; Optional[dict]:&#10;    &quot;&quot;&quot;&#10;    손상된 JSON을 복구하려고 시도&#10;    &quot;&quot;&quot;&#10;    print(&quot;손상된 JSON 복구 시도...&quot;)&#10;&#10;    try:&#10;        # 1. 흔한 문제들 수정&#10;        fixed_str = json_str&#10;&#10;        # 잘못된 키 이름 패턴 수정 (예: &quot;텍스트key&quot;: -&gt; &quot;key&quot;:)&#10;        import re&#10;        fixed_str = re.sub(r'&quot;[^&quot;]*[가-힣][^&quot;]*([a-zA-Z_][a-zA-Z0-9_]*)&quot;:', r'&quot;\1&quot;:', fixed_str)&#10;&#10;        # 중간에 끊어진 문자열 + 키 패턴 수정&#10;        fixed_str = re.sub(r'&quot;[^&quot;]*&quot;([a-zA-Z_][a-zA-Z0-9_]*)&quot;:', r'&quot;, &quot;\1&quot;:', fixed_str)&#10;&#10;        # 잘 구분된 쉼표 패턴 수정&#10;        fixed_str = re.sub(r',\s*}', '}', fixed_str)&#10;        fixed_str = re.sub(r',\s*]', ']', fixed_str)&#10;&#10;        # 2. JSON 파싱 재시도&#10;        result = json.loads(fixed_str)&#10;        print(&quot;JSON 복구 성공!&quot;)&#10;        return result&#10;&#10;    except json.JSONDecodeError as e:&#10;        print(f&quot;JSON 복구 실패: {e}&quot;)&#10;&#10;        # 3. 부분 복구 시도 - 유효한 필드만 추출&#10;        try:&#10;            partial_data = {}&#10;&#10;            # 간단한 키-값 쌍 추출&#10;            simple_patterns = [&#10;                (r'&quot;([^&quot;]+)&quot;:\s*&quot;([^&quot;]*)&quot;', str),  # 문자열 값&#10;                (r'&quot;([^&quot;]+)&quot;:\s*(\d+(?:\.\d+)?)', float),  # 숫자 값&#10;                (r'&quot;([^&quot;]+)&quot;:\s*(true|false)', bool),  # 불린 값&#10;            ]&#10;&#10;            for pattern, value_type in simple_patterns:&#10;                matches = re.findall(pattern, json_str)&#10;                for key, value in matches:&#10;                    try:&#10;                        if value_type == bool:&#10;                            partial_data[key] = value.lower() == 'true'&#10;                        elif value_type == float:&#10;                            partial_data[key] = float(value)&#10;                        else:&#10;                            partial_data[key] = value&#10;                    except:&#10;                        continue&#10;&#10;            if partial_data:&#10;                print(f&quot;부분 JSON 복구 성공: {len(partial_data)}개 필드&quot;)&#10;                return partial_data&#10;&#10;        except Exception as e:&#10;            print(f&quot;부분 복구도 실패: {e}&quot;)&#10;&#10;    return None" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/llm_news/news_analyzer.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/llm_news/news_analyzer.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;뉴스 및 리서치 분석 모듈 - Gemini API 활용&#10;&quot;&quot;&quot;&#10;&#10;from typing import List, Dict&#10;import json&#10;import logging&#10;from datetime import datetime&#10;from _1st_stage_news_analysis_LLM import client, model_name, ask_question_to_gemini_cache, json_match, create_news_analysis_prompt, create_research_reports_analysis_prompt, create_individual_news_analysis_prompt&#10;&#10;logger = logging.getLogger(__name__)&#10;&#10;class NewsAnalyzer:&#10;    &quot;&quot;&quot;뉴스 및 리서치 리포트 분석기&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.client = client&#10;        self.model_name = model_name&#10;&#10;    def analyze_news_sentiment(self, news_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        뉴스 감정 분석&#10;&#10;        Args:&#10;            news_data: 뉴스 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 감정 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not news_data:&#10;            return {&quot;error&quot;: &quot;분석할 뉴스 데이터가 없습니다.&quot;}&#10;&#10;        # 뉴스 제목과 내용을 하나의 텍스트로 결합&#10;        combined_text = &quot;&quot;&#10;        for idx, news in enumerate(news_data, 1):&#10;            combined_text += f&quot;\n\n--- 뉴스 {idx} ---\n&quot;&#10;            combined_text += f&quot;제목: {news.get('title', '')}\n&quot;&#10;            # content가 비어있으면 제목만 사용&#10;            content = news.get('content', '')&#10;            if content.strip():&#10;                combined_text += f&quot;내용: {content[:500]}...\n&quot;&#10;            else:&#10;                combined_text += f&quot;내용: 제목 참조\n&quot;&#10;&#10;        # 통합된 프롬프트 생성 함수 사용&#10;        prompt = create_news_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = json_match(response)&#10;&#10;            if parsed_result:&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['news_count'] = len(news_data)&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;overall_sentiment&quot;: &quot;neutral&quot;,&#10;                    &quot;sentiment_score&quot;: 0,&#10;                    &quot;key_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_impact&quot;: &quot;JSON 파싱 실패로 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;뉴스 분석 중 오류가 발생했습니다.&quot;,&#10;                    &quot;investment_signals&quot;: &quot;hold&quot;,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;news_count&quot;: len(news_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 감정 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오류 발생: {str(e)}&quot;}&#10;&#10;    def analyze_research_reports(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        리서치 리포트 분석 (개선된 버전 - 카테고리별 분석 포괄)&#10;&#10;        Args:&#10;            reports_data: 리포트 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 리포트 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트 데이터가 없습니다.&quot;}&#10;&#10;        # 카테고리별로 리포트 분류&#10;        categorized_reports = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'unknown')&#10;            if category not in categorized_reports:&#10;                categorized_reports[category] = []&#10;            categorized_reports[category].append(report)&#10;&#10;        # 분석용 텍스트 생성&#10;        combined_text = self._format_reports_for_analysis(categorized_reports)&#10;&#10;        # 통합된 리서치 리포트 분석 프롬프트 사용&#10;        prompt = create_research_reports_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = json_match(response)&#10;&#10;            if parsed_result:&#10;                # 필수 항목이 모두 포함되었는지 검증&#10;                required_fields = [&#10;                    'category_summary', 'top_mentioned_stocks', 'key_industries',&#10;                    'investment_themes', 'market_outlook', 'risk_factors',&#10;                    'opportunities', 'analyst_consensus', 'summary'&#10;                ]&#10;&#10;                # 누락된 필드가 있으면 기본값으로 채우기&#10;                for field in required_fields:&#10;                    if field not in parsed_result:&#10;                        if field == 'category_summary':&#10;                            parsed_result[field] = {&quot;종목분석&quot;: &quot;분석 데이터 부족&quot;, &quot;산업분석&quot;: &quot;분석 데이터 부족&quot;, &quot;시황정보&quot;: &quot;분석 데이터 부족&quot;, &quot;투자정보&quot;: &quot;분석 데이터 부족&quot;}&#10;                        elif field in ['top_mentioned_stocks', 'key_industries', 'investment_themes', 'risk_factors', 'opportunities']:&#10;                            parsed_result[field] = [&quot;데이터 부족&quot;]&#10;                        elif field == 'market_outlook':&#10;                            parsed_result[field] = &quot;neutral&quot;&#10;                        elif field in ['analyst_consensus', 'summary']:&#10;                            parsed_result[field] = &quot;분석 데이터가 부족합니다.&quot;&#10;&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['reports_count'] = len(reports_data)&#10;                parsed_result['category_counts'] = {&#10;                    cat: len(reports) for cat, reports in categorized_reports.items()&#10;                }&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;category_summary&quot;: {&#10;                        &quot;종목분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;산업분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;시황정보&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;투자정보&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;&#10;                    },&#10;                    &quot;top_mentioned_stocks&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;key_industries&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;investment_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_outlook&quot;: &quot;neutral&quot;,&#10;                    &quot;risk_factors&quot;: [&quot;JSON 파싱 실패로 리스크 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;opportunities&quot;: [&quot;JSON 파싱 실패로 기회 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;analyst_consensus&quot;: &quot;리포트 분석 중 JSON 파싱 오류가 발생했습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;전체 리포트 분석 중 오류가 발생하여 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;raw_response&quot;: response,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;reports_count&quot;: len(reports_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;리서치 리포트 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오류 발생: {str(e)}&quot;}&#10;&#10;    def _format_reports_for_analysis(self, categorized_reports: Dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        카테고리별 리포트 데이터를 분석할 텍스트로 포맷팅&#10;        &quot;&quot;&quot;&#10;        formatted_text = &quot;&quot;&#10;&#10;        category_names = {&#10;            'stock_analysis': '종목분석 리포트',&#10;            'industry_analysis': '산업분석 리포트',&#10;            'market_info': '시황정보 리포트',&#10;            'investment_info': '투자정보 리포트',&#10;            '종목분석': '종목분석 리포트',&#10;            '산업분석': '산업분석 리포트',&#10;            '시황정보': '시황정보 리포트',&#10;            '투자정보': '투자정보 리포트'&#10;        }&#10;&#10;        for category, reports in categorized_reports.items():&#10;            if reports:&#10;                display_name = category_names.get(category, category)&#10;                formatted_text += f&quot;\n\n=== {display_name} ===\n&quot;&#10;&#10;                for idx, report in enumerate(reports, 1):&#10;                    formatted_text += f&quot;\n{idx}. 제목: {report.get('title', 'N/A')}\n&quot;&#10;&#10;                    if report.get('provider'):&#10;                        formatted_text += f&quot;   증권사: {report['provider']}\n&quot;&#10;&#10;                    if report.get('company'):&#10;                        formatted_text += f&quot;   종목: {report['company']}\n&quot;&#10;&#10;                    # summary가 비어있으면 제목으로 대체&#10;                    summary = report.get('summary', '')&#10;                    if summary.strip() and summary != &quot;요약 내용을 찾을 수 없습니다.&quot;:&#10;                        formatted_text += f&quot;   요약: {summary[:200]}...\n&quot;&#10;                    else:&#10;                        formatted_text += f&quot;   요약: 제목 참조\n&quot;&#10;&#10;                    if report.get('publish_date'):&#10;                        formatted_text += f&quot;   날짜: {report['publish_date']}\n&quot;&#10;&#10;        return formatted_text&#10;&#10;    def analyze_comprehensive_with_categories(self, crawled_data: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 상세 분석을 포함한 종합 분석&#10;&#10;        Args:&#10;            crawled_data: 크롤링된 전체 데이터&#10;&#10;        Returns:&#10;            Dict: 종합 분석 결과 (카테고리별 세부 분석 포함)&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;카테고리별 상세 종합 분석 시작&quot;)&#10;&#10;        # 전체 뉴스 감정 분석&#10;        news_analysis = self.analyze_news_sentiment(crawled_data.get('main_news', []))&#10;        logger.info(&quot;뉴스 감정 분석 완료&quot;)&#10;&#10;        # 리서치 리포트 분석&#10;        reports_analysis = self.analyze_research_reports(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;리서치 리포트 분석 완료&quot;)&#10;&#10;        # 카테고리별 심화 분석&#10;        category_insights = self._generate_category_insights(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;카테고리별 심화 분석 완료&quot;)&#10;&#10;        # 일일 종합 리포트 생성&#10;        daily_report = self.generate_enhanced_daily_report(news_analysis, reports_analysis, category_insights)&#10;        logger.info(&quot;일일 종합 리포트 생성 완료&quot;)&#10;&#10;        return {&#10;            'news_analysis': news_analysis,&#10;            'reports_analysis': reports_analysis,&#10;            'category_insights': category_insights,&#10;            'daily_report': daily_report,&#10;            'meta': {&#10;                'total_analyzed': len(crawled_data.get('main_news', [])) + len(crawled_data.get('research_reports', [])),&#10;                'analysis_completed_at': datetime.now().isoformat()&#10;            }&#10;        }&#10;&#10;    def _generate_category_insights(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 심화 인사이트 생성&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트가 없습니다.&quot;}&#10;&#10;        # 카테고리별 통계&#10;        category_stats = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'Unknown')&#10;            if category not in category_stats:&#10;                category_stats[category] = {&#10;                    'count': 0,&#10;                    'firms': set(),&#10;                    'stocks': set(),&#10;                    'recent_titles': []&#10;                }&#10;&#10;            category_stats[category]['count'] += 1&#10;&#10;            if report.get('provider'):&#10;                category_stats[category]['firms'].add(report['provider'])&#10;&#10;            if report.get('company'):&#10;                category_stats[category]['stocks'].add(report['company'])&#10;&#10;            if report.get('title'):&#10;                category_stats[category]['recent_titles'].append(report['title'])&#10;&#10;        # 통계를 JSON 직렬화 가능한 형태로 변환&#10;        formatted_stats = {}&#10;        for category, stats in category_stats.items():&#10;            formatted_stats[category] = {&#10;                'count': stats['count'],&#10;                'active_firms': list(stats['firms'])[:5],  # 최대 5개&#10;                'mentioned_stocks': list(stats['stocks'])[:10],  # 최대 10개&#10;                'sample_titles': stats['recent_titles'][:3]  # 최대 3개&#10;            }&#10;&#10;        return {&#10;            'category_statistics': formatted_stats,&#10;            'total_categories': len(category_stats),&#10;            'most_active_category': max(category_stats.keys(), key=lambda k: category_stats[k]['count']) if category_stats else None,&#10;            'generated_at': datetime.now().isoformat()&#10;        }&#10;&#10;    def generate_enhanced_daily_report(self, news_analysis: Dict, reports_analysis: Dict, category_insights: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리 인사이트를 포함한 개선된 일일 리포트 생성&#10;        &quot;&quot;&quot;&#10;        # 간단한 점수 계산&#10;        try:&#10;            sentiment_score = news_analysis.get('sentiment_score', 50)&#10;            market_sentiment_score = max(1, min(10, int(sentiment_score / 10)))&#10;&#10;            return {&#10;                'market_sentiment_score': market_sentiment_score,&#10;                'confidence_level': 7,  # 기본 신뢰도&#10;                'summary': f&quot;뉴스 {news_analysis.get('news_count', 0)}개, 리포트 {reports_analysis.get('reports_count', 0)}개 분석 완료&quot;,&#10;                'recommendations': [&#10;                    &quot;시장 동향을 지속적으로 모니터링하세요&quot;,&#10;                    &quot;리스크 관리를 철저히 하세요&quot;&#10;                ],&#10;                'generated_at': datetime.now().isoformat()&#10;            }&#10;        except Exception as e:&#10;            logger.error(f&quot;일일 리포트 생성 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;일일 리포트 생성 실패: {str(e)}&quot;}&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;뉴스 및 리서치 분석 모듈 - Gemini API 활용 (통합 크롤링 기능 포함)&#10;&quot;&quot;&quot;&#10;&#10;from typing import List, Dict&#10;import json&#10;import logging&#10;from datetime import datetime&#10;import requests&#10;from bs4 import BeautifulSoup&#10;import time&#10;import re&#10;import os&#10;from dotenv import load_dotenv&#10;&#10;# 환경 변수 로드&#10;load_dotenv()&#10;&#10;from google import genai&#10;from google.genai import types&#10;&#10;# 로��� 설정&#10;logging.basicConfig(level=logging.INFO, format='%(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;class IntegratedNewsAnalyzer:&#10;    &quot;&quot;&quot;통합 뉴스 및 리서치 크롤링 &amp; 분석기&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.model_name = &quot;gemini-2.0-flash-001&quot;&#10;        self.client = genai.Client(api_key=os.getenv(&quot;GOOGLE_AI_API_KEY&quot;))&#10;        self.headers = {&#10;            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'&#10;        }&#10;        self.session = requests.Session()&#10;        self.session.headers.update(self.headers)&#10;&#10;    def ask_question_to_gemini_cache(self, prompt, max_retries=5, retry_delay=5):&#10;        &quot;&quot;&quot;&#10;        Gemini API를 사용하여 질문에 대한 답변을 얻습니다.&#10;        뉴스 분석에 최적화된 버전입니다.&#10;        &quot;&quot;&quot;&#10;        start_time = time.time()&#10;&#10;        for attempt in range(max_retries):&#10;            try:&#10;                # API 키 확인&#10;                api_key = os.getenv(&quot;GOOGLE_AI_API_KEY&quot;)&#10;                if not api_key:&#10;                    raise Exception(&quot;GOOGLE_AI_API_KEY 환경 변수가 설정되지 않았습니다.&quot;)&#10;&#10;                # Gemini API 호출&#10;                response = self.client.models.generate_content(&#10;                    model=self.model_name,&#10;                    contents=prompt,&#10;                    config=types.GenerateContentConfig(&#10;                        temperature=0.3,&#10;                        max_output_tokens=2048&#10;                    )&#10;                )&#10;&#10;                return response.text&#10;&#10;            except Exception as e:&#10;                error_msg = str(e).lower()&#10;                print(f&quot;API 오류 (시도 {attempt + 1}/{max_retries}): {e}&quot;)&#10;&#10;                if hasattr(e, 'code') and e.code == 503:&#10;                    print(f&quot;⏳ API 사용량 한도 초과 (시도 {attempt + 1}/{max_retries}). {retry_delay}초 후 재시도...&quot;)&#10;                    time.sleep(retry_delay)&#10;                    continue&#10;&#10;                if attempt == max_retries - 1:&#10;                    return f&quot;API 호출 실패: {e}&quot;&#10;&#10;                time.sleep(retry_delay)&#10;&#10;        return &quot;모든 재시도 실패&quot;&#10;&#10;    def json_match(self, text):&#10;        &quot;&quot;&quot;&#10;        텍스트에서 JSON 객체를 추출하는 함수&#10;        &quot;&quot;&quot;&#10;        try:&#10;            # 중괄호 패턴 매칭&#10;            pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'&#10;            matches = re.findall(pattern, text, re.DOTALL)&#10;&#10;            for match in matches:&#10;                try:&#10;                    return json.loads(match)&#10;                except json.JSONDecodeError:&#10;                    continue&#10;&#10;            # 백틱으로 감싸진 JSON 찾기&#10;            json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'&#10;            matches = re.findall(json_pattern, text, re.DOTALL)&#10;&#10;            for match in matches:&#10;                try:&#10;                    return json.loads(match)&#10;                except json.JSONDecodeError:&#10;                    continue&#10;&#10;            return None&#10;        except Exception as e:&#10;            print(f&quot;JSON 파싱 오류: {e}&quot;)&#10;            return None&#10;&#10;    def create_news_analysis_prompt(self, news_text):&#10;        &quot;&quot;&quot;뉴스 분석용 프롬프트 생성&quot;&quot;&quot;&#10;        return f&quot;&quot;&quot;&#10;다음 뉴스들을 분석하여 JSON 형태로 결과를 제공해주세요:&#10;&#10;{news_text}&#10;&#10;분석 결과를 다음 JSON 형식으로 정확히 제공해주세요:&#10;&#10;{{&#10;  &quot;overall_sentiment&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;sentiment_score&quot;: 0-100,&#10;  &quot;key_themes&quot;: [&quot;주요 테마1&quot;, &quot;주요 테마2&quot;],&#10;  &quot;market_impact&quot;: &quot;시장에 미치는 영향 분석&quot;,&#10;  &quot;summary&quot;: &quot;전체 뉴스 요약&quot;,&#10;  &quot;investment_signals&quot;: &quot;buy/sell/hold&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;&#10;    def create_research_reports_analysis_prompt(self, reports_text):&#10;        &quot;&quot;&quot;리서치 리포트 분석용 프롬프트 생성&quot;&quot;&quot;&#10;        return f&quot;&quot;&quot;&#10;다음 리서치 리포트들을 분석하여 JSON 형태로 결과를 제공해주세요:&#10;&#10;{reports_text}&#10;&#10;분석 ���과를 다음 JSON 형식으로 정확히 제공해주세요:&#10;&#10;{{&#10;  &quot;category_summary&quot;: {{&#10;    &quot;종목분석&quot;: &quot;종목분석 요약&quot;,&#10;    &quot;산업분석&quot;: &quot;산업분석 요약&quot;, &#10;    &quot;시황정보&quot;: &quot;시황정보 요약&quot;,&#10;    &quot;투자정보&quot;: &quot;투자정보 요약&quot;&#10;  }},&#10;  &quot;top_mentioned_stocks&quot;: [&quot;종목1&quot;, &quot;종목2&quot;, &quot;종목3&quot;],&#10;  &quot;key_industries&quot;: [&quot;업종1&quot;, &quot;업종2&quot;, &quot;업종3&quot;],&#10;  &quot;investment_themes&quot;: [&quot;투자테마1&quot;, &quot;투자테마2&quot;],&#10;  &quot;market_outlook&quot;: &quot;positive/negative/neutral&quot;,&#10;  &quot;risk_factors&quot;: [&quot;리스크1&quot;, &quot;리스크2&quot;],&#10;  &quot;opportunities&quot;: [&quot;기회1&quot;, &quot;기회2&quot;],&#10;  &quot;analyst_consensus&quot;: &quot;애널리스트 consensus&quot;,&#10;  &quot;summary&quot;: &quot;전체 리포트 종합 ���약&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;&#10;    def crawl_and_analyze_all(self, news_section_id: str = &quot;101&quot;, news_limit: int = 20, reports_limit: int = 10) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        뉴스와 리포트를 크롤링하고 분석하여 통합된 결과 반환&#10;&#10;        Args:&#10;            news_section_id: 네이버 뉴스 섹션 ID (101: 정치, 102: 경제, 103: 사회 등)&#10;            news_limit: 크롤링할 뉴스 개수&#10;            reports_limit: 크롤링할 리포트 개수 (카테고리별)&#10;&#10;        Returns:&#10;            Dict: 통합�� 크롤링 및 분석 결과&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;=&quot; * 60)&#10;        logger.info(&quot;통합 뉴스 &amp; 리서치 크롤��� 및 분석 시작&quot;)&#10;        logger.info(&quot;=&quot; * 60)&#10;&#10;        # 1. 뉴스 크롤링&#10;        logger.info(&quot; 네이버 뉴스 헤드라인 크롤링 시작...&quot;)&#10;        news_data = self._crawl_naver_news(news_section_id, news_limit)&#10;&#10;        # 2. 리서치 리포트 크롤링&#10;        logger.info(&quot; 네이버 증권 리서치 리포트 크롤링 시작...&quot;)&#10;        reports_data = self._crawl_research_reports(reports_limit)&#10;&#10;        # 3. 데이터 분석&#10;        logger.info(&quot; AI 기반 데이터 분석 시작...&quot;)&#10;&#10;        # 뉴스 분석&#10;        news_analysis = {}&#10;        if news_data:&#10;            news_analysis = self.analyze_news_sentiment(news_data)&#10;&#10;        # 리포트 분석&#10;        reports_analysis = {}&#10;        if reports_data:&#10;            reports_analysis = self.analyze_research_reports(reports_data)&#10;&#10;        # 4. 통합 결과 생성&#10;        integrated_result = {&#10;            'metadata': {&#10;                'crawled_at': datetime.now().isoformat(),&#10;                'news_section_id': news_section_id,&#10;                'news_count': len(news_data),&#10;                'reports_count': len(reports_data),&#10;                'source': 'integrated_news_research_crawler'&#10;            },&#10;            'news': {&#10;                'data': news_data,&#10;                'analysis': news_analysis&#10;            },&#10;            'research_reports': {&#10;                'data': reports_data,&#10;                'analysis': reports_analysis&#10;            },&#10;            'summary': {&#10;                'total_items': len(news_data) + len(reports_data),&#10;                'successful_news_crawl': len([n for n in news_data if n.get('content') and len(n['content']) &gt; 50]),&#10;                'successful_reports_crawl': len([r for r in reports_data if r.get('summary') and len(r['summary']) &gt; 50]),&#10;                'news_sentiment': news_analysis.get('overall_sentiment', 'unknown'),&#10;                'reports_outlook': reports_analysis.get('overall_outlook', 'unknown')&#10;            }&#10;        }&#10;&#10;        # 5. JSON 파일 저장&#10;        logger.info(&quot; 통합 결과 JSON 파일 저장 중...&quot;)&#10;        filename = self._save_integrated_json(integrated_result)&#10;&#10;        if filename:&#10;            logger.info(f&quot;✅ 통합 JSON 파일 저장 완료: {filename}&quot;)&#10;            integrated_result['saved_file'] = filename&#10;&#10;        logger.info(&quot;=&quot; * 60)&#10;        logger.info(&quot;통합 크롤링 및 분석 완료&quot;)&#10;        logger.info(&quot;=&quot; * 60)&#10;&#10;        return integrated_result&#10;&#10;    def _crawl_naver_news(self, section_id: str, limit: int) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;네이버 뉴스 헤드라인 크롤링 (test_headline_crawler.py 기능 통합)&quot;&quot;&quot;&#10;        news_list = []&#10;&#10;        try:&#10;            url = f&quot;https://news.naver.com/section/{section_id}&quot;&#10;            logger.info(f&quot; 네이버 뉴스 섹션 크롤링: {url}&quot;)&#10;&#10;            response = self.session.get(url)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            # 헤드라인 뉴스 링크 수집&#10;            headline_selectors = [&#10;                '.section_headline .sa_text_title',&#10;                '.section_latest .sa_text_title',&#10;                '.sa_text_title',&#10;                'a[href*=&quot;/article/&quot;]',&#10;            ]&#10;&#10;            news_links = []&#10;            for selector in headline_selectors:&#10;                try:&#10;                    elements = soup.select(selector)&#10;                    for element in elements:&#10;                        if element.name == 'a':&#10;                            link = element&#10;                        else:&#10;                            link = element.find_parent('a') or element.find('a')&#10;&#10;                        if link and link.get('href'):&#10;                            href = link.get('href')&#10;                            title = link.get_text(strip=True)&#10;&#10;                            if '/article/' in href and title and len(title) &gt; 5:&#10;                                full_url = href if href.startswith('http') else f&quot;https://news.naver.com{href}&quot;&#10;                                news_links.append({&#10;                                    'title': title,&#10;                                    'url': full_url&#10;                                })&#10;&#10;                        if len(news_links) &gt;= limit:&#10;                            break&#10;&#10;                    if len(news_links) &gt;= limit:&#10;                        break&#10;&#10;                except Exception as e:&#10;                    logger.warning(f&quot;선택자 '{selector}' 처리 중 오류: {e}&quot;)&#10;                    continue&#10;&#10;            # 중복 제거&#10;            seen_urls = set()&#10;            unique_news = []&#10;            for news in news_links:&#10;                if news['url'] not in seen_urls:&#10;                    seen_urls.add(news['url'])&#10;                    unique_news.append(news)&#10;                    if len(unique_news) &gt;= limit:&#10;                        break&#10;&#10;            logger.info(f&quot; 헤드라인 뉴스 {len(unique_news)}개 발견&quot;)&#10;&#10;            # 각 뉴스의 본문 크롤링&#10;            for idx, news_item in enumerate(unique_news):&#10;                try:&#10;                    logger.info(f&quot; 뉴스 본문 크롤링... ({idx + 1}/{len(unique_news)})&quot;)&#10;&#10;                    content_data = self._crawl_news_content(news_item['url'])&#10;&#10;                    news_data = {&#10;                        'id': idx + 1,&#10;                        'title': news_item['title'],&#10;                        'url': news_item['url'],&#10;                        'content': content_data.get('content', ''),&#10;                        'summary': content_data.get('summary', ''),&#10;                        'publish_date': content_data.get('publish_date', ''),&#10;                        'media': content_data.get('media', ''),&#10;                        'category': f'section_{section_id}',&#10;                        'crawled_at': datetime.now().isoformat()&#10;                    }&#10;&#10;                    news_list.append(news_data)&#10;                    logger.info(f&quot;✅ 뉴스 수집 완료 ({len(news_list)}/{limit}): '{news_item['title'][:50]}...'&quot;)&#10;&#10;                    time.sleep(1)  # 서버 부하 방지&#10;&#10;                except Exception as e:&#10;                    logger.error(f&quot;뉴스 본문 크롤링 실패: {e}&quot;)&#10;                    # 기본 정보라도 저장&#10;                    news_data = {&#10;                        'id': idx + 1,&#10;                        'title': news_item['title'],&#10;                        'url': news_item['url'],&#10;                        'content': '',&#10;                        'summary': '',&#10;                        'publish_date': '',&#10;                        'media': '',&#10;                        'category': f'section_{section_id}',&#10;                        'crawled_at': datetime.now().isoformat()&#10;                    }&#10;                    news_list.append(news_data)&#10;                    continue&#10;&#10;            logger.info(f&quot; 뉴스 크롤링 완료: 총 {len(news_list)}개 수집&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 크롤링 중 오류: {e}&quot;)&#10;&#10;        return news_list&#10;&#10;    def _crawl_news_content(self, url: str) -&gt; Dict:&#10;        &quot;&quot;&quot;개별 뉴스 기사 본문 크롤링 (art_crawl 방식)&quot;&quot;&quot;&#10;        try:&#10;            response = self.session.get(url, timeout=10)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            # 정확한 네이버 뉴스 선택자 사용&#10;            title_selector = &quot;#title_area &gt; span&quot;&#10;            date_selector = &quot;#ct &gt; div.media_end_head.go_trans &gt; div.media_end_head_info.nv_notrans &gt; div.media_end_head_info_datestamp &gt; div:nth-child(1) &gt; span&quot;&#10;            main_selector = &quot;#dic_area&quot;&#10;&#10;            # 제목 추출&#10;            title = ''&#10;            try:&#10;                title_elements = soup.select(title_selector)&#10;                title_lst = [t.get_text(strip=True) for t in title_elements]&#10;                title = &quot;&quot;.join(title_lst)&#10;            except:&#10;                pass&#10;&#10;            # 발행일 추출&#10;            publish_date = ''&#10;            try:&#10;                date_elements = soup.select(date_selector)&#10;                date_lst = [d.get_text(strip=True) for d in date_elements]&#10;                publish_date = &quot;&quot;.join(date_lst)&#10;            except:&#10;                # 대체 선택자들&#10;                fallback_selectors = [&#10;                    '.media_end_head_info_datestamp_time',&#10;                    'span[data-date-time]',&#10;                    '.author em'&#10;                ]&#10;                for selector in fallback_selectors:&#10;                    try:&#10;                        date_element = soup.select_one(selector)&#10;                        if date_element:&#10;                            publish_date = date_element.get_text(strip=True)&#10;                            break&#10;                    except:&#10;                        continue&#10;&#10;            # 본문 추출&#10;            content = ''&#10;            try:&#10;                main_elements = soup.select(main_selector)&#10;                main_lst = []&#10;                for m in main_elements:&#10;                    # 불필요한 요소 제거&#10;                    for script in m([&quot;script&quot;, &quot;style&quot;, &quot;iframe&quot;]):&#10;                        script.decompose()&#10;&#10;                    # 네이버 뉴스 특화 불필요 요소 제거&#10;                    unwanted_selectors = [&#10;                        '.end_photo_org',&#10;                        '.media_end_head_journalist',&#10;                        '.media_end_summary',&#10;                        'strong.media_end_summary'&#10;                    ]&#10;                    for unwanted_selector in unwanted_selectors:&#10;                        for unwanted in m.select(unwanted_selector):&#10;                            unwanted.decompose()&#10;&#10;                    m_text = m.get_text(strip=True)&#10;                    if m_text:&#10;                        main_lst.append(m_text)&#10;&#10;                content = &quot; &quot;.join(main_lst)&#10;&#10;                # 텍스트 정리&#10;                content = re.sub(r'\s+', ' ', content).strip()&#10;&#10;                # 불필요한 문구 제거&#10;                unwanted_phrases = [&#10;                    &quot;무단전재 및 재배포 금지&quot;,&#10;                    &quot;저작권자&quot;,&#10;                    &quot;ⓒ&quot;,&#10;                    &quot;Copyright&quot;&#10;                ]&#10;                for phrase in unwanted_phrases:&#10;                    if phrase in content:&#10;                        content = content.split(phrase)[0].strip()&#10;&#10;            except:&#10;                # ���체 선택자들&#10;                fallback_selectors = [&#10;                    '#articleBodyContents',&#10;                    '.go_trans._article_content',&#10;                    '._article_body_contents'&#10;                ]&#10;                for selector in fallback_selectors:&#10;                    try:&#10;                        content_element = soup.select_one(selector)&#10;                        if content_element:&#10;                            for script in content_element([&quot;script&quot;, &quot;style&quot;]):&#10;                                script.decompose()&#10;                            content = content_element.get_text(strip=True)&#10;                            if len(content) &gt; 50:&#10;                                break&#10;                    except:&#10;                        continue&#10;&#10;            # 요약 생성&#10;            summary = ''&#10;            if content:&#10;                sentences = content.split('.')&#10;                if sentences:&#10;                    summary = sentences[0][:200] + ('...' if len(sentences[0]) &gt; 200 else '')&#10;&#10;            # 언론사 추출&#10;            media = ''&#10;            media_selectors = [&#10;                '.media_end_head_top_logo img',&#10;                '.press_logo img',&#10;                '.media_end_head_top_logo_text'&#10;            ]&#10;            for selector in media_selectors:&#10;                try:&#10;                    media_element = soup.select_one(selector)&#10;                    if media_element:&#10;                        if media_element.name == 'img':&#10;                            media = media_element.get('alt', '')&#10;                        else:&#10;                            media = media_element.get_text(strip=True)&#10;                        if media:&#10;                            break&#10;                except:&#10;                    continue&#10;&#10;            return {&#10;                'content': content,&#10;                'summary': summary,&#10;                'publish_date': publish_date,&#10;                'media': media,&#10;                'title': title&#10;            }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 본문 크롤링 실패 ({url}): {e}&quot;)&#10;            return {&#10;                'content': '',&#10;                'summary': '',&#10;                'publish_date': '',&#10;                'media': '',&#10;                'title': ''&#10;            }&#10;&#10;    def _crawl_research_reports(self, limit: int) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;네이버 증권 리서치 리포트 크롤링&quot;&quot;&quot;&#10;        all_reports = []&#10;&#10;        # 네이버 금융 리서치 카테고리별 URL&#10;        category_urls = {&#10;            '종목분석': &quot;https://finance.naver.com/research/company_list.naver&quot;,&#10;            '산업분석': &quot;https://finance.naver.com/research/industry_list.naver&quot;,&#10;            '시황정보': &quot;https://finance.naver.com/research/market_info_list.naver&quot;,&#10;            '투자정보': &quot;https://finance.naver.com/research/invest_list.naver&quot;&#10;        }&#10;&#10;        logger.info(f&quot; 네이버 금융 리서치에서 각 카테���리별 최신 리포트 {limit}개씩 수집&quot;)&#10;&#10;        for category_name, category_url in category_urls.items():&#10;            try:&#10;                logger.info(f&quot; {category_name} 리포트 크롤링... ({category_url})&quot;)&#10;&#10;                response = self.session.get(category_url, timeout=10)&#10;                response.raise_for_status()&#10;&#10;                soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;                # 리포트 테이블 찾기&#10;                table_selectors = [&#10;                    'table.type_1',&#10;                    'table.type_2',&#10;                    'table[summary*=&quot;리포트&quot;]',&#10;                    'div.box_type_m table',&#10;                    'table'&#10;                ]&#10;&#10;                table = None&#10;                for selector in table_selectors:&#10;                    table = soup.select_one(selector)&#10;                    if table and len(table.find_all('tr')) &gt; 1:&#10;                        logger.info(f&quot;{category_name} 리포트 테이블 발견&quot;)&#10;                        break&#10;&#10;                if not table:&#10;                    logger.warning(f&quot;{category_name}: 리포트 테이블을 찾을 수 없습니다.&quot;)&#10;                    continue&#10;&#10;                rows = table.find_all('tr')&#10;                data_rows = rows[1:] if rows[0].find('th') else rows&#10;&#10;                count = 0&#10;                for row in data_rows:&#10;                    if count &gt;= limit:&#10;                        break&#10;&#10;                    cells = row.find_all('td')&#10;                    if len(cells) &lt; 2:&#10;                        continue&#10;&#10;                    try:&#10;                        # 제목과 링크 찾기&#10;                        title = &quot;&quot;&#10;                        link = &quot;&quot;&#10;&#10;                        for cell in cells[:3]:&#10;                            a_tag = cell.find('a')&#10;                            if a_tag and a_tag.get('href'):&#10;                                title = a_tag.get_text(strip=True)&#10;                                link = a_tag.get('href')&#10;&#10;                                if len(title) &gt; 5 and not title.isdigit():&#10;                                    break&#10;&#10;                        if not title or not link:&#10;                            continue&#10;&#10;                        # URL 정리&#10;                        if link.startswith('/'):&#10;                            link = &quot;https://finance.naver.com&quot; + link&#10;                        elif not link.startswith('http'):&#10;                            link = f&quot;https://finance.naver.com/research/{link}&quot;&#10;&#10;                        # 발행일 추출&#10;                        publish_date = &quot;&quot;&#10;                        for cell in reversed(cells):&#10;                            cell_text = cell.get_text(strip=True)&#10;                            if self._is_valid_date(cell_text):&#10;                                publish_date = cell_text&#10;                                break&#10;&#10;                        # 증권사 정보 추출&#10;                        provider = &quot;&quot;&#10;                        for cell in cells:&#10;                            cell_text = cell.get_text(strip=True)&#10;                            if any(keyword in cell_text for keyword in ['증권', '투자', '자산', '캐피탈', 'Securities']):&#10;                                provider = cell_text&#10;                                break&#10;&#10;                        # 리포트 상세 내용 크롤링 (간소화)&#10;                        content = self._crawl_report_content(link)&#10;&#10;                        report_data = {&#10;                            'id': len(all_reports) + 1,&#10;                            'title': title,&#10;                            'link': link,&#10;                            'summary': content if content else title,  # 본문을 summary로 저장&#10;                            'provider': provider,&#10;                            'publish_date': publish_date if publish_date else 'unknown',&#10;                            'category_name': category_name,&#10;                            'category_key': category_name.lower(),&#10;                            'crawled_at': datetime.now().isoformat()&#10;                        }&#10;&#10;                        all_reports.append(report_data)&#10;                        count += 1&#10;&#10;                        logger.info(f&quot;✅ {category_name}: '{title[:30]}...' 리포트 수집 완료 ({count}/{limit})&quot;)&#10;&#10;                        time.sleep(1)  # 서버 부하 방지&#10;&#10;                    except Exception as e:&#10;                        logger.error(f&quot;{category_name} 리포트 처리 중 오류: {e}&quot;)&#10;                        continue&#10;&#10;                logger.info(f&quot; {category_name}: {count}개 리포트 수집 완료&quot;)&#10;                time.sleep(2)  # 카테고리 간 딜레이&#10;&#10;            except Exception as e:&#10;                logger.error(f&quot;{category_name} 카테고리 크롤링 중 오류: {e}&quot;)&#10;                continue&#10;&#10;        logger.info(f&quot; 전체 리서치 리포트 크롤링 완료: 총 {len(all_reports)}개 수집&quot;)&#10;        return all_reports&#10;&#10;    def _crawl_report_content(self, link: str) -&gt; str:&#10;        &quot;&quot;&quot;리포트 상세 내용 크롤링 (간소화)&quot;&quot;&quot;&#10;        try:&#10;            response = self.session.get(link, timeout=10)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            content_selectors = [&#10;                'div.view_cnt',&#10;                'td.view_cnt',&#10;                'div.report_content',&#10;                'div.content',&#10;                'div[class*=&quot;content&quot;]'&#10;            ]&#10;&#10;            for selector in content_selectors:&#10;                try:&#10;                    content_div = soup.select_one(selector)&#10;                    if content_div:&#10;                        # 불필요한 요소 제거&#10;                        for unwanted in content_div.select('script, style, iframe, .ad, .advertisement'):&#10;                            unwanted.decompose()&#10;&#10;                        text = content_div.get_text(separator=' ', strip=True)&#10;&#10;                        # 텍스트 정리&#10;                        text = re.sub(r'\s+', ' ', text).strip()&#10;&#10;                        if len(text) &gt; 100:&#10;                            return text[:500] + &quot;...&quot; if len(text) &gt; 500 else text&#10;                except:&#10;                    continue&#10;&#10;            return &quot;&quot;&#10;&#10;        except Exception as e:&#10;            logger.warning(f&quot;리포트 본문 크롤링 실패 ({link}): {e}&quot;)&#10;            return &quot;&quot;&#10;&#10;    def _is_valid_date(self, text: str) -&gt; bool:&#10;        &quot;&quot;&quot;날짜 형식 검증&quot;&quot;&quot;&#10;        import re&#10;        date_patterns = [&#10;            r'\d{4}[-./]\d{1,2}[-./]\d{1,2}',&#10;            r'\d{1,2}[-./]\d{1,2}[-./]\d{2,4}',&#10;            r'\d{4}년\s*\d{1,2}월\s*\d{1,2}일',&#10;            r'\d{2}\.\d{2}\.\d{2}',&#10;            r'\d{4}\.\d{2}\.\d{2}',&#10;        ]&#10;&#10;        for pattern in date_patterns:&#10;            if re.search(pattern, text):&#10;                return True&#10;        return False&#10;&#10;    def _save_integrated_json(self, data: Dict, filename: str = None) -&gt; str:&#10;        &quot;&quot;&quot;통합 결과를 JSON 파일로 저장&quot;&quot;&quot;&#10;        if filename is None:&#10;            timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;            filename = f&quot;integrated_news_research_{timestamp}.json&quot;&#10;&#10;        try:&#10;            with open(filename, 'w', encoding='utf-8') as f:&#10;                json.dump(data, f, ensure_ascii=False, indent=2)&#10;&#10;            logger.info(f&quot; 통합 JSON 파일 저장 완료: {filename}&quot;)&#10;            return filename&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;JSON 파일 저장 실패: {e}&quot;)&#10;            return None&#10;&#10;    def analyze_news_sentiment(self, news_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        뉴스 감정 분석&#10;&#10;        Args:&#10;            news_data: 뉴스 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 감정 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not news_data:&#10;            return {&quot;error&quot;: &quot;분석할 뉴스 데이터가 없습니다.&quot;}&#10;&#10;        # 뉴스 제목과 내용을 하나의 텍스트로 결합&#10;        combined_text = &quot;&quot;&#10;        for idx, news in enumerate(news_data, 1):&#10;            combined_text += f&quot;\n\n--- 뉴스 {idx} ---\n&quot;&#10;            combined_text += f&quot;제목: {news.get('title', '')}\n&quot;&#10;            # content가 비어있으면 제목만 사용&#10;            content = news.get('content', '')&#10;            if content.strip():&#10;                combined_text += f&quot;내용: {content[:500]}...\n&quot;&#10;            else:&#10;                combined_text += f&quot;내용: 제목 참조\n&quot;&#10;&#10;        # 통합된 프롬프트 생성 함수 사용&#10;        prompt = self.create_news_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = self.ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = self.json_match(response)&#10;&#10;            if parsed_result:&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['news_count'] = len(news_data)&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;overall_sentiment&quot;: &quot;neutral&quot;,&#10;                    &quot;sentiment_score&quot;: 0,&#10;                    &quot;key_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_impact&quot;: &quot;JSON 파싱 실패로 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;뉴스 분석 중 오류가 발생했습니다.&quot;,&#10;                    &quot;investment_signals&quot;: &quot;hold&quot;,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;news_count&quot;: len(news_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 감정 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오류 발생: {str(e)}&quot;}&#10;&#10;    def analyze_research_reports(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        리서치 리포트 분석 (개선된 버전 - 카테고리별 분석 포괄)&#10;&#10;        Args:&#10;            reports_data: 리포트 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 리포트 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트 데이터가 없습니다.&quot;}&#10;&#10;        # 카테고리별로 리포트 분류&#10;        categorized_reports = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'unknown')&#10;            if category not in categorized_reports:&#10;                categorized_reports[category] = []&#10;            categorized_reports[category].append(report)&#10;&#10;        # 분석용 텍스트 생성&#10;        combined_text = self._format_reports_for_analysis(categorized_reports)&#10;&#10;        # 통합된 리서치 리포트 분석 프롬프트 사용&#10;        prompt = self.create_research_reports_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = self.ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = self.json_match(response)&#10;&#10;            if parsed_result:&#10;                # 필수 항목이 모두 포함되었는지 검증&#10;                required_fields = [&#10;                    'category_summary', 'top_mentioned_stocks', 'key_industries',&#10;                    'investment_themes', 'market_outlook', 'risk_factors',&#10;                    'opportunities', 'analyst_consensus', 'summary'&#10;                ]&#10;&#10;                # 누락된 필드가 있으면 기본값으로 채우기&#10;                for field in required_fields:&#10;                    if field not in parsed_result:&#10;                        if field == 'category_summary':&#10;                            parsed_result[field] = {&quot;종목분석&quot;: &quot;분석 데이터 부족&quot;, &quot;산업분석&quot;: &quot;분석 데이터 부족&quot;, &quot;시황정보&quot;: &quot;분석 데이터 부족&quot;, &quot;투자정보&quot;: &quot;분석 데이터 부족&quot;}&#10;                        elif field in ['top_mentioned_stocks', 'key_industries', 'investment_themes', 'risk_factors', 'opportunities']:&#10;                            parsed_result[field] = [&quot;데이터 부족&quot;]&#10;                        elif field == 'market_outlook':&#10;                            parsed_result[field] = &quot;neutral&quot;&#10;                        elif field in ['analyst_consensus', 'summary']:&#10;                            parsed_result[field] = &quot;분석 데이터가 부족합니다.&quot;&#10;&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['reports_count'] = len(reports_data)&#10;                parsed_result['category_counts'] = {&#10;                    cat: len(reports) for cat, reports in categorized_reports.items()&#10;                }&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;category_summary&quot;: {&#10;                        &quot;종목분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;산업분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;시황정보&quot;: &quot;JSON 파싱 ���패로 분석할 수 없습니다.&quot;,&#10;                        &quot;투자정보&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;&#10;                    },&#10;                    &quot;top_mentioned_stocks&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;key_industries&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;investment_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_outlook&quot;: &quot;neutral&quot;,&#10;                    &quot;risk_factors&quot;: [&quot;JSON 파싱 실패로 리스크 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;opportunities&quot;: [&quot;JSON 파싱 실패로 기회 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;analyst_consensus&quot;: &quot;리포트 분석 중 JSON 파싱 오류가 발생했습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;전체 리포트 분석 중 오류가 발생하여 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;raw_response&quot;: response,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;reports_count&quot;: len(reports_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;리서치 리포트 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오��� 발생: {str(e)}&quot;}&#10;&#10;    def _format_reports_for_analysis(self, categorized_reports: Dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        카테고리별 리포트 데이터를 분석할 텍스트로 포맷팅&#10;        &quot;&quot;&quot;&#10;        formatted_text = &quot;&quot;&#10;&#10;        category_names = {&#10;            'stock_analysis': '종목분석 리포트',&#10;            'industry_analysis': '산업분석 리포트',&#10;            'market_info': '시황정보 리포트',&#10;            'investment_info': '투자정보 리포트',&#10;            '종목분석': '종목분석 리포트',&#10;            '산업분석': '산업분석 리포트',&#10;            '시황정보': '시황정보 리포트',&#10;            '투자정보': '투자정보 리포트'&#10;        }&#10;&#10;        for category, reports in categorized_reports.items():&#10;            if reports:&#10;                display_name = category_names.get(category, category)&#10;                formatted_text += f&quot;\n\n=== {display_name} ===\n&quot;&#10;&#10;                for idx, report in enumerate(reports, 1):&#10;                    formatted_text += f&quot;\n{idx}. 제목: {report.get('title', 'N/A')}\n&quot;&#10;&#10;                    if report.get('provider'):&#10;                        formatted_text += f&quot;   증권사: {report['provider']}\n&quot;&#10;&#10;                    # summary가 비어있으면 제목으로 대체&#10;                    summary = report.get('summary', '')&#10;                    if summary.strip() and summary != &quot;요약 내용을 찾을 수 없습니다.&quot;:&#10;                        formatted_text += f&quot;   요약: {summary[:200]}...\n&quot;&#10;                    else:&#10;                        formatted_text += f&quot;   요약: 제목 참조\n&quot;&#10;&#10;                    if report.get('publish_date'):&#10;                        formatted_text += f&quot;   날짜: {report['publish_date']}\n&quot;&#10;&#10;        return formatted_text&#10;&#10;    def analyze_comprehensive_with_categories(self, crawled_data: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 상세 분석을 포함한 종합 분석&#10;&#10;        Args:&#10;            crawled_data: 크롤링된 전체 데이터&#10;&#10;        Returns:&#10;            Dict: 종합 분석 결과 (카테고리별 세부 분석 포함)&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;카테고리별 상세 종합 분석 시작&quot;)&#10;&#10;        # 전체 뉴스 감정 분석&#10;        news_analysis = self.analyze_news_sentiment(crawled_data.get('main_news', []))&#10;        logger.info(&quot;뉴스 감정 분석 완료&quot;)&#10;&#10;        # 리서치 리포트 분석&#10;        reports_analysis = self.analyze_research_reports(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;리서치 리포트 분석 완료&quot;)&#10;&#10;        # 카테고리별 심화 분석&#10;        category_insights = self._generate_category_insights(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;카테고리별 심화 분석 완료&quot;)&#10;&#10;        # 일일 종합 리포트 생성&#10;        daily_report = self.generate_enhanced_daily_report(news_analysis, reports_analysis, category_insights)&#10;        logger.info(&quot;일일 종합 리포트 생성 완료&quot;)&#10;&#10;        return {&#10;            'news_analysis': news_analysis,&#10;            'reports_analysis': reports_analysis,&#10;            'category_insights': category_insights,&#10;            'daily_report': daily_report,&#10;            'meta': {&#10;                'total_analyzed': len(crawled_data.get('main_news', [])) + len(crawled_data.get('research_reports', [])),&#10;                'analysis_completed_at': datetime.now().isoformat()&#10;            }&#10;        }&#10;&#10;    def _generate_category_insights(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 심화 인사이트 생성&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트가 없습니다.&quot;}&#10;&#10;        # 카테고리별 통계&#10;        category_stats = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'Unknown')&#10;            if category not in category_stats:&#10;                category_stats[category] = {&#10;                    'count': 0,&#10;                    'firms': set(),&#10;                    'stocks': set(),&#10;                    'recent_titles': []&#10;                }&#10;&#10;            category_stats[category]['count'] += 1&#10;&#10;            if report.get('provider'):&#10;                category_stats[category]['firms'].add(report['provider'])&#10;&#10;            if report.get('title'):&#10;                category_stats[category]['recent_titles'].append(report['title'])&#10;&#10;        # 통계를 JSON 직렬화 가능한 ���태로 변환&#10;        formatted_stats = {}&#10;        for category, stats in category_stats.items():&#10;            formatted_stats[category] = {&#10;                'count': stats['count'],&#10;                'active_firms': list(stats['firms'])[:5],  # 최대 5개&#10;                'mentioned_stocks': list(stats['stocks'])[:10],  # 최대 10개&#10;                'sample_titles': stats['recent_titles'][:3]  # 최대 3개&#10;            }&#10;&#10;        return {&#10;            'category_statistics': formatted_stats,&#10;            'total_categories': len(category_stats),&#10;            'most_active_category': max(category_stats.keys(), key=lambda k: category_stats[k]['count']) if category_stats else None,&#10;            'generated_at': datetime.now().isoformat()&#10;        }&#10;&#10;    def generate_enhanced_daily_report(self, news_analysis: Dict, reports_analysis: Dict, category_insights: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리 인사이트를 포함한 개선된 일일 리포트 생성&#10;        &quot;&quot;&quot;&#10;        # 간단한 점수 계산&#10;        try:&#10;            sentiment_score = news_analysis.get('sentiment_score', 50)&#10;            market_sentiment_score = max(1, min(10, int(sentiment_score / 10)))&#10;&#10;            return {&#10;                'market_sentiment_score': market_sentiment_score,&#10;                'confidence_level': 7,  # 기본 신뢰도&#10;                'summary': f&quot;뉴스 {news_analysis.get('news_count', 0)}개, 리포트 {reports_analysis.get('reports_count', 0)}개 분석 완료&quot;,&#10;                'recommendations': [&#10;                    &quot;시장 동향을 지속적으로 모니터링하세요&quot;,&#10;                    &quot;리스크 관리를 철저히 하세요&quot;&#10;                ],&#10;                'generated_at': datetime.now().isoformat()&#10;            }&#10;        except Exception as e:&#10;            logger.error(f&quot;일일 리포트 생성 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;일일 리포트 생성 실패: {str(e)}&quot;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/llm_news/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/llm_news/requirements.txt" />
              <option name="updatedContent" value="requests==2.31.0&#10;beautifulsoup4==4.12.2&#10;python-dotenv==1.0.0&#10;google-generativeai==0.3.2" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/llm_news/test_headline_crawler.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/llm_news/test_headline_crawler.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;네이버 뉴스 헤드라인 크롤러 테스트 스크립트&#10;정치 섹션(101)에서 헤드라인 뉴스 20개를 크롤링하고 JSON으로 저장&#10;&quot;&quot;&quot;&#10;&#10;import sys&#10;import os&#10;import json&#10;from datetime import datetime&#10;&#10;# 현재 디렉토리를 Python 경로에 추가&#10;sys.path.append(os.path.dirname(os.path.abspath(__file__)))&#10;&#10;from news_crawler import NaverNewsHeadlineCrawler&#10;&#10;def main():&#10;    &quot;&quot;&quot;메인 실행 함수&quot;&quot;&quot;&#10;    print(&quot;=&quot; * 60)&#10;    print(&quot;네이버 뉴스 헤드라인 크롤러 테스트&quot;)&#10;    print(&quot;=&quot; * 60)&#10;&#10;    # 크롤러 인스턴스 생성&#10;    crawler = NaverNewsHeadlineCrawler()&#10;&#10;    # 정치 섹션(101)에서 헤드라인 뉴스 20개 크롤링&#10;    print(&quot; 네이버 뉴스 정치 섹션에서 헤드라인 뉴스 20개 크롤링 시작...&quot;)&#10;    print(&quot; URL: https://news.naver.com/section/101&quot;)&#10;    print()&#10;&#10;    try:&#10;        # 뉴스 크롤링 실행&#10;        news_list = crawler.get_headline_news(section_id=&quot;101&quot;, limit=20)&#10;&#10;        if news_list:&#10;            print(f&quot;✅ 크롤링 완료! 총 {len(news_list)}개의 뉴스를 수집했습니다.&quot;)&#10;            print()&#10;&#10;            # 결과 미리보기&#10;            print(&quot; 수집된 뉴스 미리보기:&quot;)&#10;            print(&quot;-&quot; * 60)&#10;            for i, news in enumerate(news_list[:3], 1):  # 처음 3개만 미리보기&#10;                print(f&quot;{i}. {news['title']}&quot;)&#10;                print(f&quot;    발행일: {news['publish_date']}&quot;)&#10;                print(f&quot;    언론사: {news['media']}&quot;)&#10;                print(f&quot;    본문 길이: {len(news['content'])}자&quot;)&#10;                if news['content']:&#10;                    preview = news['content'][:100] + &quot;...&quot; if len(news['content']) &gt; 100 else news['content']&#10;                    print(f&quot;    본문 미리보기: {preview}&quot;)&#10;                print()&#10;&#10;            if len(news_list) &gt; 3:&#10;                print(f&quot;... 및 {len(news_list) - 3}개 더&quot;)&#10;                print()&#10;&#10;            # JSON 파일로 저장&#10;            print(&quot; JSON 파일로 저장 중...&quot;)&#10;            filename = crawler.save_to_json(news_list)&#10;&#10;            if filename:&#10;                print(f&quot;✅ JSON 파일 저장 완료: {filename}&quot;)&#10;&#10;                # 저장된 파일 정보 출력&#10;                file_size = os.path.getsize(filename) / 1024  # KB 단위&#10;                print(f&quot; 파일 크기: {file_size:.1f} KB&quot;)&#10;&#10;                # JSON 구조 미리보기&#10;                print()&#10;                print(&quot; JSON 구조 미리보기:&quot;)&#10;                print(&quot;-&quot; * 40)&#10;&#10;                with open(filename, 'r', encoding='utf-8') as f:&#10;                    data = json.load(f)&#10;&#10;                print(f&quot; 메타데이터:&quot;)&#10;                print(f&quot;   - 총 뉴스 개수: {data['metadata']['total_count']}&quot;)&#10;                print(f&quot;   - 크롤링 시간: {data['metadata']['crawled_at']}&quot;)&#10;                print(f&quot;   - 데이터 소스: {data['metadata']['source']}&quot;)&#10;                print()&#10;&#10;                print(f&quot; 첫 번째 뉴스 구조:&quot;)&#10;                if data['news']:&#10;                    first_news = data['news'][0]&#10;                    for key, value in first_news.items():&#10;                        if key == 'content' and isinstance(value, str) and len(value) &gt; 50:&#10;                            print(f&quot;   - {key}: {value[:50]}... ({len(value)}자)&quot;)&#10;                        else:&#10;                            print(f&quot;   - {key}: {value}&quot;)&#10;            else:&#10;                print(&quot;❌ JSON 파일 저장 실패&quot;)&#10;&#10;        else:&#10;            print(&quot;❌ 뉴스 크롤링에 실패했습니다.&quot;)&#10;            print(&quot;네트워크 연결이나 웹사이트 구조 변경을 확인해주세요.&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;❌ 크롤링 중 오류 발생: {e}&quot;)&#10;        import traceback&#10;        print(&quot;상세 오류 정보:&quot;)&#10;        traceback.print_exc()&#10;&#10;    print()&#10;    print(&quot;=&quot; * 60)&#10;    print(&quot;테스트 완료&quot;)&#10;    print(&quot;=&quot; * 60)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;네이버 뉴스 헤드라인 크롤러 테스트 스크립트 - 개선된 버전&#10;정치 섹션(101)에서 헤드라인 뉴스 20개를 크롤링하고 JSON으로 저장&#10;&quot;&quot;&quot;&#10;&#10;import sys&#10;import os&#10;import json&#10;from datetime import datetime&#10;&#10;# 현재 디렉토리를 Python 경로에 추가&#10;sys.path.append(os.path.dirname(os.path.abspath(__file__)))&#10;&#10;from news_crawler import NaverNewsHeadlineCrawler&#10;&#10;def test_single_article():&#10;    &quot;&quot;&quot;단일 기사 크롤링 테스트&quot;&quot;&quot;&#10;    print(&quot;=&quot; * 60)&#10;    print(&quot;단일 기사 크롤링 테스트 (art_crawl 함수)&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    &#10;    # 크롤러 인스턴스 생성&#10;    crawler = NaverNewsHeadlineCrawler()&#10;    &#10;    # 테스트용 네이버 뉴스 URL (임시로 정치 섹션에서 하나 가져와서 테스트)&#10;    test_url = &quot;https://news.naver.com/section/100&quot;  # 정치 섹션&#10;    &#10;    try:&#10;        # 먼저 헤드라인에서 첫 번째 기사 URL 가져오기&#10;        print(&quot; 정치 섹션에서 테스트용 기사 URL 가져오는 중...&quot;)&#10;        &#10;        response = crawler.session.get(test_url)&#10;        response.raise_for_status()&#10;        &#10;        from bs4 import BeautifulSoup&#10;        soup = BeautifulSoup(response.content, 'html.parser')&#10;        &#10;        # 첫 번째 기사 링크 찾기&#10;        article_links = soup.select('a[href*=&quot;/article/&quot;]')&#10;        &#10;        if article_links:&#10;            first_article_url = article_links[0].get('href')&#10;            if not first_article_url.startswith('http'):&#10;                first_article_url = f&quot;https://news.naver.com{first_article_url}&quot;&#10;            &#10;            print(f&quot; 테스트 기사 URL: {first_article_url}&quot;)&#10;            print()&#10;            &#10;            # art_crawl 함수로 크롤링&#10;            print(&quot; art_crawl 함수로 크롤링 시작...&quot;)&#10;            result = crawler.art_crawl(first_article_url)&#10;            &#10;            print(&quot;✅ 크롤링 완료!&quot;)&#10;            print()&#10;            print(&quot; 크롤링 결과:&quot;)&#10;            print(&quot;-&quot; * 40)&#10;            print(f&quot;제목: {result['title']}&quot;)&#10;            print(f&quot;날짜: {result['date']}&quot;)&#10;            print(f&quot;본문 길이: {len(result['main'])}자&quot;)&#10;            &#10;            if result['main']:&#10;                preview = result['main'][:200] + &quot;...&quot; if len(result['main']) &gt; 200 else result['main']&#10;                print(f&quot;본문 미리보기: {preview}&quot;)&#10;            else:&#10;                print(&quot;⚠️ 본문을 가져오지 못했습니다.&quot;)&#10;            &#10;            print()&#10;            print(&quot; 전체 결과 JSON:&quot;)&#10;            print(json.dumps(result, ensure_ascii=False, indent=2))&#10;            &#10;        else:&#10;            print(&quot;❌ 테스트용 기사를 찾을 수 없습니다.&quot;)&#10;    &#10;    except Exception as e:&#10;        print(f&quot;❌ 테스트 중 오류 발생: {e}&quot;)&#10;&#10;def main():&#10;    &quot;&quot;&quot;메인 실행 함수&quot;&quot;&quot;&#10;    print(&quot;=&quot; * 60)&#10;    print(&quot;네이버 뉴스 헤드라인 크롤러 테스트 - 개선된 버전&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    &#10;    # 먼저 단일 기사 테스트&#10;    test_single_article()&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;헤드라인 뉴스 20개 크롤링 테스트&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    &#10;    # 크롤러 인스턴스 생성&#10;    crawler = NaverNewsHeadlineCrawler()&#10;    &#10;    # 정치 섹션(101)에서 헤드라인 뉴스 20개 크롤링&#10;    print(&quot; 네이버 뉴스 정치 섹션에서 헤드라인 뉴스 20개 크롤링 시작...&quot;)&#10;    print(&quot; URL: https://news.naver.com/section/101&quot;)&#10;    print()&#10;    &#10;    try:&#10;        # 뉴스 크롤링 실행&#10;        news_list = crawler.get_headline_news(section_id=&quot;101&quot;, limit=20)&#10;        &#10;        if news_list:&#10;            print(f&quot;✅ 크롤링 완료! 총 {len(news_list)}개의 뉴스를 수집했습니다.&quot;)&#10;            print()&#10;            &#10;            # 본문이 제대로 크롤링된 뉴스 개수 확인&#10;            content_success_count = sum(1 for news in news_list if news.get('content') and len(news['content']) &gt; 50)&#10;            print(f&quot; 본문 크롤링 성공률: {content_success_count}/{len(news_list)} ({content_success_count/len(news_list)*100:.1f}%)&quot;)&#10;            print()&#10;            &#10;            # 결과 미리보기&#10;            print(&quot; 수집된 뉴스 미리보기:&quot;)&#10;            print(&quot;-&quot; * 60)&#10;            for i, news in enumerate(news_list[:5], 1):  # 처음 5개만 미리보기&#10;                print(f&quot;{i}. {news['title']}&quot;)&#10;                print(f&quot;    발행일: {news['publish_date']}&quot;)&#10;                print(f&quot;    언론사: {news['media']}&quot;)&#10;                print(f&quot;    본문 길이: {len(news['content'])}자&quot;)&#10;                if news['content']:&#10;                    preview = news['content'][:150] + &quot;...&quot; if len(news['content']) &gt; 150 else news['content']&#10;                    print(f&quot;    본문 미리보기: {preview}&quot;)&#10;                else:&#10;                    print(&quot;   ⚠️ 본문을 가져오지 못했습니다.&quot;)&#10;                print()&#10;            &#10;            if len(news_list) &gt; 5:&#10;                print(f&quot;... 및 {len(news_list) - 5}개 더&quot;)&#10;                print()&#10;            &#10;            # JSON 파일로 저장&#10;            print(&quot; JSON 파일로 저장 중...&quot;)&#10;            filename = crawler.save_to_json(news_list)&#10;            &#10;            if filename:&#10;                print(f&quot;✅ JSON 파일 저장 완료: {filename}&quot;)&#10;                &#10;                # 저장된 파일 정보 출력&#10;                file_size = os.path.getsize(filename) / 1024  # KB 단위&#10;                print(f&quot; 파일 크기: {file_size:.1f} KB&quot;)&#10;                &#10;                # 성공적으로 크롤링된 뉴스들의 통계&#10;                print()&#10;                print(&quot; 크롤링 통계:&quot;)&#10;                print(&quot;-&quot; * 40)&#10;                &#10;                total_content_length = sum(len(news.get('content', '')) for news in news_list)&#10;                avg_content_length = total_content_length / len(news_list) if news_list else 0&#10;                &#10;                print(f&quot;   - 전체 뉴스 수: {len(news_list)}개&quot;)&#10;                print(f&quot;   - 본문 크롤링 성공: {content_success_count}개&quot;)&#10;                print(f&quot;   - 평균 본문 길이: {avg_content_length:.0f}자&quot;)&#10;                print(f&quot;   - 총 본문 길이: {total_content_length:,}자&quot;)&#10;                &#10;                # 언론사별 분포&#10;                media_count = {}&#10;                for news in news_list:&#10;                    media = news.get('media', '알 수 없음')&#10;                    if media:&#10;                        media_count[media] = media_count.get(media, 0) + 1&#10;                &#10;                if media_count:&#10;                    print(f&quot;   - 언론사별 분포:&quot;)&#10;                    for media, count in sorted(media_count.items(), key=lambda x: x[1], reverse=True)[:5]:&#10;                        print(f&quot;     • {media}: {count}개&quot;)&#10;            else:&#10;                print(&quot;❌ JSON 파일 저장 실패&quot;)&#10;        &#10;        else:&#10;            print(&quot;❌ 뉴스 크롤링에 실패했습니다.&quot;)&#10;            print(&quot;네트워크 연결이나 웹사이트 구조 변경을 확인해주세요.&quot;)&#10;    &#10;    except Exception as e:&#10;        print(f&quot;❌ 크롤링 중 오류 발생: {e}&quot;)&#10;        import traceback&#10;        print(&quot;상세 오류 정보:&quot;)&#10;        traceback.print_exc()&#10;    &#10;    print()&#10;    print(&quot;=&quot; * 60)&#10;    print(&quot;테스트 완료&quot;)&#10;    print(&quot;=&quot; * 60)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;네이버 증권 뉴스 크롤링 및 AI 분석 시스템 메인 실행 파일&#10;&quot;&quot;&quot;&#10;&#10;import json&#10;import logging&#10;from datetime import datetime&#10;from news_crawler import NaverStockNewsCrawler&#10;from news_analyzer import NewsAnalyzer&#10;&#10;# 로깅 설정 (간단한 진행 상황 표시)&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(message)s',&#10;    handlers=[&#10;        logging.StreamHandler()&#10;    ]&#10;)&#10;&#10;logger = logging.getLogger(__name__)&#10;&#10;class StockNewsAnalysisSystem:&#10;    &quot;&quot;&quot;주식 뉴스 분석 시스템 메인 클래스&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.crawler = NaverStockNewsCrawler()&#10;        self.analyzer = NewsAnalyzer()&#10;&#10;    def run_daily_analysis(self, news_limit: int = 10, reports_limit: int = 10) -&gt; dict:&#10;        &quot;&quot;&quot;&#10;        일일 뉴스 분석 실행 (카테고리별 상세 분석 포함)&#10;&#10;        Args:&#10;            news_limit: 크롤링할 뉴스 개수&#10;            reports_limit: 각 카테고리별로 크롤링할 리포트 개수&#10;&#10;        Returns:&#10;            Dict: 전체 분석 결과&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;--- 일일 주식 뉴스 분석 시작 ---&quot;)&#10;&#10;        try:&#10;            # 1. 뉴스 및 리포트 크롤링&#10;            logger.info(&quot;1단계: 뉴스 및 리포트 크롤링 시작&quot;)&#10;            crawled_data = self.crawler.get_today_summary()&#10;&#10;            if crawled_data['total_count'] == 0:&#10;                logger.warning(&quot;크롤링된 데이터가 없습니다.&quot;)&#10;                return {&quot;error&quot;: &quot;크롤링된 데이터가 없습니다.&quot;}&#10;&#10;            # 크롤링 현황 출력&#10;            logger.info(f&quot;뉴스 크롤링: {len(crawled_data['main_news'])}개&quot;)&#10;            logger.info(f&quot;리포트 크롤링: {len(crawled_data['research_reports'])}개&quot;)&#10;&#10;            # 카테고리별 통계 출력&#10;            category_counts = {}&#10;            for report in crawled_data['research_reports']:&#10;                category = report.get('category_name', 'Unknown')&#10;                category_counts[category] = category_counts.get(category, 0) + 1&#10;&#10;            if category_counts:&#10;                for category, count in category_counts.items():&#10;                    logger.info(f&quot;  {category}: {count}개&quot;)&#10;&#10;            # 2. AI 분석 수행 (카테고리별 상세 분석 포함)&#10;            logger.info(&quot;Gemini API 분석 시작&quot;)&#10;            analysis_result = self.analyzer.analyze_comprehensive_with_categories(crawled_data)&#10;&#10;            # 3. 결과 저장&#10;            self._save_results(crawled_data, analysis_result)&#10;&#10;            logger.info(&quot;분석 완료&quot;)&#10;&#10;            return {&#10;                'crawled_data': crawled_data,&#10;                'analysis_result': analysis_result,&#10;                'status': 'success'&#10;            }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;분석 시스템 실행 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;시스템 실행 중 오류: {str(e)}&quot;}&#10;&#10;    def _save_results(self, crawled_data: dict, analysis_result: dict):&#10;        &quot;&quot;&quot;분석 결과를 파일로 저장&quot;&quot;&quot;&#10;        timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;&#10;        # 크롤링 데이터 저장&#10;        with open(f'crawled_data_{timestamp}.json', 'w', encoding='utf-8') as f:&#10;            json.dump(crawled_data, f, ensure_ascii=False, indent=2)&#10;&#10;        # 분석 결과 저장&#10;        with open(f'analysis_result_{timestamp}.json', 'w', encoding='utf-8') as f:&#10;            json.dump(analysis_result, f, ensure_ascii=False, indent=2)&#10;&#10;        logger.info(f&quot;결과 파일 저장 완료: crawled_data_{timestamp}.json, analysis_result_{timestamp}.json&quot;)&#10;&#10;    def print_summary(self, result: dict):&#10;        &quot;&quot;&quot;분석 결과 요약 출력&quot;&quot;&quot;&#10;        if 'error' in result:&#10;            print(f&quot;❌ 오류: {result['error']}&quot;)&#10;            return&#10;&#10;        analysis = result['analysis_result']&#10;&#10;        print(&quot;\n&quot; + &quot;-&quot;*50)&#10;        print(&quot;&lt;오늘의 주식 시장 분석 결과&gt;&quot;)&#10;        print(&quot;-&quot;*50)&#10;&#10;        # 뉴스 분석 요약&#10;        if 'news_analysis' in analysis:&#10;            news = analysis['news_analysis']&#10;            print(f&quot;\n 뉴스 분석 ({news.get('news_count', 0)}개)&quot;)&#10;            print(f&quot;   감정: {news.get('overall_sentiment', 'N/A')}&quot;)&#10;            print(f&quot;   점수: {news.get('sentiment_score', 'N/A')}/100&quot;)&#10;            print(f&quot;   투자신호: {news.get('investment_signals', 'N/A')}&quot;)&#10;            if 'summary' in news:&#10;                print(f&quot;   요약: {news['summary']}&quot;)&#10;&#10;        # 리포트 분석 요약 (카테고리별)&#10;        if 'reports_analysis' in analysis:&#10;            reports = analysis['reports_analysis']&#10;            print(f&quot;\n 리포트 분석 ({reports.get('reports_count', 0)}개)&quot;)&#10;            print(f&quot;   시장 전망: {reports.get('market_outlook', 'N/A')}&quot;)&#10;&#10;            if 'top_mentioned_stocks' in reports:&#10;                stocks = reports['top_mentioned_stocks'][:5]&#10;                if stocks:&#10;                    print(f&quot;   주목 종목: {', '.join(stocks)}&quot;)&#10;&#10;        # 일일 종합 분석 및 매수콜&#10;        if 'daily_report' in analysis:&#10;            daily = analysis['daily_report']&#10;            if 'error' not in daily:&#10;                print(f&quot;\n⭐ 종합 평가&quot;)&#10;                print(f&quot;   시장 감정 점수: {daily.get('market_sentiment_score', 'N/A')}/10&quot;)&#10;                print(f&quot;   신뢰도: {daily.get('confidence_level', 'N/A')}/10&quot;)&#10;                &#10;                # 매수콜 표시&#10;                sentiment_score = analysis.get('news_analysis', {}).get('sentiment_score', 50)&#10;                investment_signal = analysis.get('news_analysis', {}).get('investment_signals', 'hold')&#10;                &#10;                print(f&quot;\n 투자 추천&quot;)&#10;                if sentiment_score &gt;= 70 or investment_signal == 'buy':&#10;                    print(f&quot;    매수 추천 (감정점수: {sentiment_score}/100)&quot;)&#10;                elif sentiment_score &lt;= 30 or investment_signal == 'sell':&#10;                    print(f&quot;    매도 추천 (감정점수: {sentiment_score}/100)&quot;)&#10;                else:&#10;                    print(f&quot;   ⏸️ 관망 추천 (감정점수: {sentiment_score}/100)&quot;)&#10;                &#10;                if 'recommendations' in daily:&#10;                    print(f&quot;   권장사항:&quot;)&#10;                    for rec in daily['recommendations']:&#10;                        print(f&quot;     • {rec}&quot;)&#10;&#10;    def print_detailed_category_analysis(self, result: dict):&#10;        &quot;&quot;&quot;카테고리별 상세 분석 결과 출력&quot;&quot;&quot;&#10;        if 'error' in result or 'category_insights' not in result['analysis_result']:&#10;            return&#10;&#10;        analysis = result['analysis_result']&#10;        insights = analysis['category_insights']&#10;&#10;        if 'category_statistics' in insights:&#10;            print(f&quot;\n&quot; + &quot;-&quot;*50)&#10;            print(&quot;&lt;카테고리별 상세 분석&gt;&quot;)&#10;            print(&quot;-&quot;*50)&#10;&#10;            stats = insights['category_statistics']&#10;            for category, data in stats.items():&#10;                print(f&quot;\n {category}: {data['count']}개 리포트&quot;)&#10;                if data['mentioned_stocks']:&#10;                    print(f&quot;   주요 종목: {', '.join(data['mentioned_stocks'][:3])}&quot;)&#10;                if data['active_firms']:&#10;                    print(f&quot;   주요 증권사: {', '.join(data['active_firms'][:3])}&quot;)&#10;&#10;            if insights.get('most_active_category'):&#10;                print(f&quot;\n 가장 활발한 카테고리: {insights['most_active_category']}&quot;)&#10;&#10;        print(&quot;\n&quot; + &quot;-&quot;*50)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        # 분석 시스템 초기화 및 실행&#10;        system = StockNewsAnalysisSystem()&#10;        &#10;        print(&quot;--------------------------------------------------&quot;)&#10;        print(&quot;&lt;오늘의 주식 시장 분석 결과&gt;&quot;)&#10;        print(&quot;--------------------------------------------------&quot;)&#10;        &#10;        # 일일 분석 실행 (뉴스 20개, 리포트 각 카테고리별 5개)&#10;        result = system.run_daily_analysis(news_limit=20, reports_limit=5)&#10;        &#10;        print(&quot;\n--------------------------------------------------&quot;)&#10;        print(&quot;&lt;뉴스 크롤링 완료&gt;&quot;)&#10;        print(&quot;--------------------------------------------------&quot;)&#10;        &#10;        # 분석 결과 요약 출력&#10;        system.print_summary(result)&#10;        &#10;        # 카테고리별 상세 분석 출력&#10;        system.print_detailed_category_analysis(result)&#10;        &#10;        print(&quot;\n분석이 완료되었습니다!&quot;)&#10;        &#10;    except Exception as e:&#10;        print(f&quot;❌ 시스템 실행 중 오류 발생: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/news_analyzer.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/news_analyzer.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;뉴스 및 리서치 분석 모듈 - Gemini API 활용&#10;&quot;&quot;&quot;&#10;&#10;from typing import List, Dict&#10;import json&#10;import logging&#10;from datetime import datetime&#10;from _1st_stage_news_analysis_LLM import client, model_name, ask_question_to_gemini_cache, json_match, create_news_analysis_prompt, create_research_reports_analysis_prompt, create_individual_news_analysis_prompt&#10;&#10;logger = logging.getLogger(__name__)&#10;&#10;class NewsAnalyzer:&#10;    &quot;&quot;&quot;뉴스 및 리서치 리포트 분석기&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.client = client&#10;        self.model_name = model_name&#10;&#10;    def analyze_news_sentiment(self, news_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        뉴스 감정 분석&#10;&#10;        Args:&#10;            news_data: 뉴스 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 감정 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not news_data:&#10;            return {&quot;error&quot;: &quot;분석할 뉴스 데이터가 없습니다.&quot;}&#10;&#10;        # 뉴스 제목과 내용을 하나의 텍스트로 결합&#10;        combined_text = &quot;&quot;&#10;        for idx, news in enumerate(news_data, 1):&#10;            combined_text += f&quot;\n\n--- 뉴스 {idx} ---\n&quot;&#10;            combined_text += f&quot;제목: {news.get('title', '')}\n&quot;&#10;            # content가 비어있으면 제목만 사용&#10;            content = news.get('content', '')&#10;            if content.strip():&#10;                combined_text += f&quot;내용: {content[:500]}...\n&quot;&#10;            else:&#10;                combined_text += f&quot;내용: 제목 참조\n&quot;&#10;&#10;        # 통합된 프롬프트 생성 함수 사용&#10;        prompt = create_news_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = json_match(response)&#10;&#10;            if parsed_result:&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['news_count'] = len(news_data)&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;overall_sentiment&quot;: &quot;neutral&quot;,&#10;                    &quot;sentiment_score&quot;: 0,&#10;                    &quot;key_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_impact&quot;: &quot;JSON 파싱 실패로 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;뉴스 분석 중 오류가 발생했습니다.&quot;,&#10;                    &quot;investment_signals&quot;: &quot;hold&quot;,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;news_count&quot;: len(news_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 감정 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오류 발생: {str(e)}&quot;}&#10;&#10;    def analyze_research_reports(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        리서치 리포트 분석 (개선된 버전 - 카테고리별 분석 포괄)&#10;&#10;        Args:&#10;            reports_data: 리포트 데이터 리스트&#10;&#10;        Returns:&#10;            Dict: 리포트 분석 결과&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트 데이터가 없습니다.&quot;}&#10;&#10;        # 카테고리별로 리포트 분류&#10;        categorized_reports = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'unknown')&#10;            if category not in categorized_reports:&#10;                categorized_reports[category] = []&#10;            categorized_reports[category].append(report)&#10;&#10;        # 분석용 텍스트 생성&#10;        combined_text = self._format_reports_for_analysis(categorized_reports)&#10;&#10;        # 통합된 리서치 리포트 분석 프롬프트 사용&#10;        prompt = create_research_reports_analysis_prompt(combined_text)&#10;&#10;        try:&#10;            response = ask_question_to_gemini_cache(prompt)&#10;&#10;            parsed_result = json_match(response)&#10;&#10;            if parsed_result:&#10;                # 필수 항목이 모두 포함되었는지 검증&#10;                required_fields = [&#10;                    'category_summary', 'top_mentioned_stocks', 'key_industries',&#10;                    'investment_themes', 'market_outlook', 'risk_factors',&#10;                    'opportunities', 'analyst_consensus', 'summary'&#10;                ]&#10;&#10;                # 누락된 필드가 있으면 기본값으로 채우기&#10;                for field in required_fields:&#10;                    if field not in parsed_result:&#10;                        if field == 'category_summary':&#10;                            parsed_result[field] = {&quot;종목분석&quot;: &quot;분석 데이터 부족&quot;, &quot;산업분석&quot;: &quot;분석 데이터 부족&quot;, &quot;시황정보&quot;: &quot;분석 데이터 부족&quot;, &quot;투자정보&quot;: &quot;분석 데이터 부족&quot;}&#10;                        elif field in ['top_mentioned_stocks', 'key_industries', 'investment_themes', 'risk_factors', 'opportunities']:&#10;                            parsed_result[field] = [&quot;데이터 부족&quot;]&#10;                        elif field == 'market_outlook':&#10;                            parsed_result[field] = &quot;neutral&quot;&#10;                        elif field in ['analyst_consensus', 'summary']:&#10;                            parsed_result[field] = &quot;분석 데이터가 부족합니다.&quot;&#10;&#10;                parsed_result['analyzed_at'] = datetime.now().isoformat()&#10;                parsed_result['reports_count'] = len(reports_data)&#10;                parsed_result['category_counts'] = {&#10;                    cat: len(reports) for cat, reports in categorized_reports.items()&#10;                }&#10;                return parsed_result&#10;            else:&#10;                # JSON 파싱 실패 시 기본값 반환&#10;                return {&#10;                    &quot;category_summary&quot;: {&#10;                        &quot;종목분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;산업분석&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;시황정보&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;,&#10;                        &quot;투자정보&quot;: &quot;JSON 파싱 실패로 분석할 수 없습니다.&quot;&#10;                    },&#10;                    &quot;top_mentioned_stocks&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;key_industries&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;investment_themes&quot;: [&quot;분석 실패&quot;],&#10;                    &quot;market_outlook&quot;: &quot;neutral&quot;,&#10;                    &quot;risk_factors&quot;: [&quot;JSON 파싱 실패로 리스크 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;opportunities&quot;: [&quot;JSON 파싱 실패로 기회 분석을 제공할 수 없습니다.&quot;],&#10;                    &quot;analyst_consensus&quot;: &quot;리포트 분석 중 JSON 파싱 오류가 발생했습니다.&quot;,&#10;                    &quot;summary&quot;: &quot;전체 리포트 분석 중 오류가 발생하여 상세 분석을 제공할 수 없습니다.&quot;,&#10;                    &quot;raw_response&quot;: response,&#10;                    &quot;analyzed_at&quot;: datetime.now().isoformat(),&#10;                    &quot;reports_count&quot;: len(reports_data),&#10;                    &quot;error&quot;: &quot;JSON 파싱 실패&quot;&#10;                }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;리서치 리포트 분석 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;분석 중 오류 발생: {str(e)}&quot;}&#10;&#10;    def _format_reports_for_analysis(self, categorized_reports: Dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        카테고리별 리포트 데이터를 분석할 텍스트로 포맷팅&#10;        &quot;&quot;&quot;&#10;        formatted_text = &quot;&quot;&#10;&#10;        category_names = {&#10;            'stock_analysis': '종목분석 리포트',&#10;            'industry_analysis': '산업분석 리포트',&#10;            'market_info': '시황정보 리포트',&#10;            'investment_info': '투자정보 리포트',&#10;            '종목분석': '종목분석 리포트',&#10;            '산업분석': '산업분석 리포트',&#10;            '시황정보': '시황정보 리포트',&#10;            '투자정보': '투자정보 리포트'&#10;        }&#10;&#10;        for category, reports in categorized_reports.items():&#10;            if reports:&#10;                display_name = category_names.get(category, category)&#10;                formatted_text += f&quot;\n\n=== {display_name} ===\n&quot;&#10;&#10;                for idx, report in enumerate(reports, 1):&#10;                    formatted_text += f&quot;\n{idx}. 제목: {report.get('title', 'N/A')}\n&quot;&#10;&#10;                    if report.get('provider'):&#10;                        formatted_text += f&quot;   증권사: {report['provider']}\n&quot;&#10;&#10;                    if report.get('company'):&#10;                        formatted_text += f&quot;   종목: {report['company']}\n&quot;&#10;&#10;                    # summary가 비어있으면 제목으로 대체&#10;                    summary = report.get('summary', '')&#10;                    if summary.strip() and summary != &quot;요약 내용을 찾을 수 없습니다.&quot;:&#10;                        formatted_text += f&quot;   요약: {summary[:200]}...\n&quot;&#10;                    else:&#10;                        formatted_text += f&quot;   요약: 제목 참조\n&quot;&#10;&#10;                    if report.get('publish_date'):&#10;                        formatted_text += f&quot;   날짜: {report['publish_date']}\n&quot;&#10;&#10;        return formatted_text&#10;&#10;    def analyze_comprehensive_with_categories(self, crawled_data: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 상세 분석을 포함한 종합 분석&#10;&#10;        Args:&#10;            crawled_data: 크롤링된 전체 데이터&#10;&#10;        Returns:&#10;            Dict: 종합 분석 결과 (카테고리별 세부 분석 포함)&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;카테고리별 상세 종합 분석 시작&quot;)&#10;&#10;        # 전체 뉴스 감정 분석&#10;        news_analysis = self.analyze_news_sentiment(crawled_data.get('main_news', []))&#10;        logger.info(&quot;뉴스 감정 분석 완료&quot;)&#10;&#10;        # 리서치 리포트 분석&#10;        reports_analysis = self.analyze_research_reports(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;리서치 리포트 분석 완료&quot;)&#10;&#10;        # 카테고리별 심화 분석&#10;        category_insights = self._generate_category_insights(crawled_data.get('research_reports', []))&#10;        logger.info(&quot;카테고리별 심화 분석 완료&quot;)&#10;&#10;        # 일일 종합 리포트 생성&#10;        daily_report = self.generate_enhanced_daily_report(news_analysis, reports_analysis, category_insights)&#10;        logger.info(&quot;일일 종합 리포트 생성 완료&quot;)&#10;&#10;        return {&#10;            'news_analysis': news_analysis,&#10;            'reports_analysis': reports_analysis,&#10;            'category_insights': category_insights,&#10;            'daily_report': daily_report,&#10;            'meta': {&#10;                'total_analyzed': len(crawled_data.get('main_news', [])) + len(crawled_data.get('research_reports', [])),&#10;                'analysis_completed_at': datetime.now().isoformat()&#10;            }&#10;        }&#10;&#10;    def _generate_category_insights(self, reports_data: List[Dict]) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리별 심화 인사이트 생성&#10;        &quot;&quot;&quot;&#10;        if not reports_data:&#10;            return {&quot;error&quot;: &quot;분석할 리포트가 없습니다.&quot;}&#10;&#10;        # 카테고리별 통계&#10;        category_stats = {}&#10;        for report in reports_data:&#10;            category = report.get('category_name', 'Unknown')&#10;            if category not in category_stats:&#10;                category_stats[category] = {&#10;                    'count': 0,&#10;                    'firms': set(),&#10;                    'stocks': set(),&#10;                    'recent_titles': []&#10;                }&#10;&#10;            category_stats[category]['count'] += 1&#10;&#10;            if report.get('provider'):&#10;                category_stats[category]['firms'].add(report['provider'])&#10;&#10;            if report.get('company'):&#10;                category_stats[category]['stocks'].add(report['company'])&#10;&#10;            if report.get('title'):&#10;                category_stats[category]['recent_titles'].append(report['title'])&#10;&#10;        # 통계를 JSON 직렬화 가능한 형태로 변환&#10;        formatted_stats = {}&#10;        for category, stats in category_stats.items():&#10;            formatted_stats[category] = {&#10;                'count': stats['count'],&#10;                'active_firms': list(stats['firms'])[:5],  # 최대 5개&#10;                'mentioned_stocks': list(stats['stocks'])[:10],  # 최대 10개&#10;                'sample_titles': stats['recent_titles'][:3]  # 최대 3개&#10;            }&#10;&#10;        return {&#10;            'category_statistics': formatted_stats,&#10;            'total_categories': len(category_stats),&#10;            'most_active_category': max(category_stats.keys(), key=lambda k: category_stats[k]['count']) if category_stats else None,&#10;            'generated_at': datetime.now().isoformat()&#10;        }&#10;&#10;    def generate_enhanced_daily_report(self, news_analysis: Dict, reports_analysis: Dict, category_insights: Dict) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        카테고리 인사이트를 포함한 개선된 일일 리포트 생성&#10;        &quot;&quot;&quot;&#10;        # 간단한 점수 계산&#10;        try:&#10;            sentiment_score = news_analysis.get('sentiment_score', 50)&#10;            market_sentiment_score = max(1, min(10, int(sentiment_score / 10)))&#10;            &#10;            return {&#10;                'market_sentiment_score': market_sentiment_score,&#10;                'confidence_level': 7,  # 기본 신뢰도&#10;                'summary': f&quot;뉴스 {news_analysis.get('news_count', 0)}개, 리포트 {reports_analysis.get('reports_count', 0)}개 분석 완료&quot;,&#10;                'recommendations': [&#10;                    &quot;시장 동향을 지속적으로 모니터링하세요&quot;,&#10;                    &quot;리스크 관리를 철저히 하세요&quot;&#10;                ],&#10;                'generated_at': datetime.now().isoformat()&#10;            }&#10;        except Exception as e:&#10;            logger.error(f&quot;일일 리포트 생성 중 오류: {e}&quot;)&#10;            return {&quot;error&quot;: f&quot;일일 리포트 생성 실패: {str(e)}&quot;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/news_crawler.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/news_crawler.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;네이버 증권 뉴스 및 리서치 크롤링 모듈&#10;&quot;&quot;&quot;&#10;&#10;import requests&#10;from bs4 import BeautifulSoup&#10;import time&#10;from datetime import datetime&#10;from typing import List, Dict&#10;import logging&#10;&#10;# 로깅 설정 (간단한 진행 상황 표시)&#10;logging.basicConfig(level=logging.INFO, format='%(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;class NaverStockNewsCrawler:&#10;    &quot;&quot;&quot;네이버 증권 뉴스 크롤러&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.base_url = &quot;https://finance.naver.com&quot;&#10;        self.headers = {&#10;            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'&#10;        }&#10;        self.session = requests.Session()&#10;        self.session.headers.update(self.headers)&#10;&#10;    def get_main_news(self, limit: int = 10) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;&#10;        네이버 증권 메인 뉴스 크롤링&#10;&#10;        Args:&#10;            limit: 가져올 뉴스 개수&#10;&#10;        Returns:&#10;            List[Dict]: 뉴스 정보 리스트&#10;        &quot;&quot;&quot;&#10;        news_list = []&#10;&#10;        try:&#10;            # 네이버 증권 뉴스 페이지 URL&#10;            url = &quot;https://finance.naver.com/news/&quot;&#10;            response = self.session.get(url)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            # 네이버 증권 뉴스 페이지의 다양한 선택자 시도&#10;            news_selectors = [&#10;                '.newslist .articleSubject a',  # 뉴스 리스트의 기사 제목&#10;                '.newsList .articleSubject a',&#10;                '.articleSubject a',            # 기본 기사 제목&#10;                '.news_list .subject a',        # 뉴스 리스트 제목&#10;                '.type2 .subject a',            # type2 스타일의 제목&#10;                '.headline_list .subject a',    # 헤드라인 리스트&#10;                '.articleSubject',              # 제목 요소 자체&#10;                'td.subject a',                 # 테이블 형태의 제목&#10;                '.tb_type1 .subject a',         # 테이블 type1의 제목&#10;                'a[href*=&quot;news_read&quot;]',         # 뉴스 읽기 링크 포함&#10;            ]&#10;&#10;            news_items = []&#10;            found_selector = None&#10;&#10;            for selector in news_selectors:&#10;                try:&#10;                    items = soup.select(selector)&#10;                    if items and len(items) &gt;= 5:  # 최소 5개 이상의 뉴스가 있어야 유효&#10;                        news_items = items[:limit]&#10;                        found_selector = selector&#10;                        logger.info(f&quot;뉴스 리스트 발견: {len(news_items)}개&quot;)&#10;                        break&#10;                except Exception as e:&#10;                    logger.error(f&quot;선택자 '{selector}' 시도 중 오류: {e}&quot;)&#10;                    continue&#10;&#10;            if not news_items:&#10;                # 모든 링크를 찾아서 뉴스 링크 필터링&#10;                logger.info(&quot;기본 선택자 실패, 모든 링크에서 뉴스 링크 찾기 시도...&quot;)&#10;                all_links = soup.find_all('a', href=True)&#10;                news_links = []&#10;&#10;                for link in all_links:&#10;                    href = link.get('href', '')&#10;                    title = link.get_text(strip=True)&#10;&#10;                    # 뉴스 링크 패턴 확인&#10;                    if ('news_read' in href or 'article_id' in href) and title and len(title) &gt; 10:&#10;                        news_links.append(link)&#10;                        if len(news_links) &gt;= limit:&#10;                            break&#10;&#10;                if news_links:&#10;                    news_items = news_links&#10;                    logger.info(f&quot;필터링을 통해 {len(news_items)}�� 뉴스 링크 발견&quot;)&#10;                else:&#10;                    logger.warning(&quot;뉴스 리스트를 찾을 수 없습니다.&quot;)&#10;                    return news_list&#10;&#10;            # 뉴스 항목 처리&#10;            for idx, item in enumerate(news_items):&#10;                try:&#10;                    # 링크 요소 확인&#10;                    if item.name == 'a':&#10;                        title_element = item&#10;                    else:&#10;                        title_element = item.find('a')&#10;&#10;                    if not title_element:&#10;                        continue&#10;&#10;                    title = title_element.get_text(strip=True)&#10;                    link = title_element.get('href')&#10;&#10;                    # 제목이 너무 짧거나 의미없는 경우 스킵&#10;                    if not title or len(title) &lt; 5:&#10;                        continue&#10;&#10;                    # URL 정리 - 스킴 문제 해결&#10;                    if link:&#10;                        if link.startswith('/'):&#10;                            # 상대 경로인 경우&#10;                            link = &quot;https://finance.naver.com&quot; + link&#10;                        elif not link.startswith('http'):&#10;                            # 프로토콜이 없는 경우&#10;                            link = &quot;https://finance.naver.com/&quot; + link.lstrip('/')&#10;&#10;                    # 상세 뉴스 내용 크롤링 (에러가 발생해도 계속 진행)&#10;                    try:&#10;                        news_detail = self._get_news_detail(link)&#10;                    except Exception as detail_error:&#10;                        logger.warning(f&quot;뉴스 상세 크롤링 실패 ('{title}'): {detail_error}&quot;)&#10;                        news_detail = {'content': '', 'publish_date': ''}&#10;&#10;                    news_data = {&#10;                        'title': title,&#10;                        'link': link,&#10;                        'content': news_detail.get('content', ''),&#10;                        'full_html': news_detail.get('full_html', ''),  # HTML 전체 내용 추가&#10;                        'publish_date': news_detail.get('publish_date', ''),&#10;                        'category': 'main_news',&#10;                        'crawled_at': datetime.now().isoformat()&#10;                    }&#10;&#10;                    news_list.append(news_data)&#10;                    logger.info(f&quot;뉴스 수집 ({len(news_list)}/{limit}): '{title[:50]}...'&quot;)&#10;&#10;                    # 원하는 개수에 도달하면 중단&#10;                    if len(news_list) &gt;= limit:&#10;                        break&#10;&#10;                    # 서버 부하 방지를 위한 딜레이&#10;                    time.sleep(0.5)  # 딜레이 단축&#10;&#10;                except Exception as e:&#10;                    logger.error(f&quot;뉴스 항목 처리 중 오류: {e}&quot;)&#10;                    continue&#10;&#10;            logger.info(f&quot;뉴스 크롤링 완료: 총 {len(news_list)}개 수집&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;메인 뉴스 크롤링 중 오류: {e}&quot;)&#10;&#10;        return news_list&#10;&#10;    def get_research_reports(self, limit: int = 10) -&gt; List[Dict]:&#10;        &quot;&quot;&quot;&#10;        네이버 증권 리서치 리포트 크롤링 (최신 리포트 수집)&#10;        각 카테고리별 개별 페이지에서 가장 최신 리포트를 수집&#10;&#10;        Args:&#10;            limit: 각 카테고리별로 가져올 리포트 개수&#10;&#10;        Returns:&#10;            List[Dict]: 리포트 정보 리스트&#10;        &quot;&quot;&quot;&#10;        all_reports = []&#10;&#10;        # 각 카테고리별 개별 페이지 URL&#10;        category_urls = {&#10;            '종목분석': f&quot;{self.base_url}/research/company_list.naver&quot;,&#10;            '산업분석': f&quot;{self.base_url}/research/industry_list.naver&quot;,&#10;            '시황정보': f&quot;{self.base_url}/research/market_info_list.naver&quot;,&#10;            '투자정보': f&quot;{self.base_url}/research/invest_list.naver&quot;&#10;        }&#10;&#10;        logger.info(f&quot;각 카테고리별 최신 리포트 {limit}개씩 수집&quot;)&#10;&#10;        for category_name, category_url in category_urls.items():&#10;            try:&#10;                logger.info(f&quot;{category_name} 최신 리포트 크롤링 시작... ({category_url})&quot;)&#10;&#10;                response = self.session.get(category_url)&#10;                response.raise_for_status()&#10;&#10;                soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;                # 테이블 찾기 - 여러 선택자 시도&#10;                table_selectors = [&#10;                    'table.type_1',&#10;                    'table.type_2',&#10;                    '.board_list table',&#10;                    '.research_list table',&#10;                    'table'&#10;                ]&#10;&#10;                table = None&#10;                for selector in table_selectors:&#10;                    table = soup.select_one(selector)&#10;                    if table:&#10;                        logger.info(f&quot;{category_name} 리포트 발견&quot;)&#10;                        break&#10;&#10;                if not table:&#10;                    logger.warning(f&quot;{category_name}: 테이블을 찾을 수 없습니다.&quot;)&#10;                    continue&#10;&#10;                # 테이블 행 추출&#10;                rows = table.find_all('tr')&#10;                if len(rows) &lt;= 1:  # 헤더만 있는 경우&#10;                    logger.warning(f&quot;{category_name}: 데이터 행이 없습니다.&quot;)&#10;                    continue&#10;&#10;                # 헤더 제외하고 데이터 행 처리&#10;                data_rows = rows[1:] if rows[0].find('th') else rows&#10;&#10;                count = 0&#10;&#10;                # 날짜 필터링 없이 최신 순으로 리��트 수집&#10;                for row in data_rows:&#10;                    if count &gt;= limit:&#10;                        break&#10;&#10;                    cells = row.find_all('td')&#10;                    if len(cells) &lt; 3:  # 최소한의 셀 개수&#10;                        continue&#10;&#10;                    try:&#10;                        # 제목과 링크 찾기&#10;                        title_link = None&#10;                        title = &quot;&quot;&#10;                        link = &quot;&quot;&#10;&#10;                        for cell in cells:&#10;                            a_tag = cell.find('a')&#10;                            if a_tag and a_tag.get('href'):&#10;                                title_link = a_tag&#10;                                title = a_tag.get_text(strip=True)&#10;                                link = a_tag.get('href')&#10;&#10;                                # URL 정리 - 올바�� 프로토콜 수정&#10;                                if not link.startswith('http'):&#10;                                    # 상대 경로인 경우 올바른 기본 경로 추가&#10;                                    if link.startswith('/'):&#10;                                        link = self.base_url + link&#10;                                    else:&#10;                                        # research 경로가 누락된 경우 추가&#10;                                        link = f&quot;{self.base_url}/research/{link}&quot;&#10;                                break&#10;&#10;                        if not title_link or not title:&#10;                            continue&#10;&#10;                        # 발행일 추출 (마지막 셀 또는 날짜가 포함된 셀)&#10;                        publish_date = &quot;&quot;&#10;                        for cell in reversed(cells):  # 뒤에서부터 찾기&#10;                            cell_text = cell.get_text(strip=True)&#10;                            # 날짜 패턴이 있는지 확인&#10;                            if any(char.isdigit() for char in cell_text) and any(sep in cell_text for sep in ['.', '-', '/']):&#10;                                publish_date = cell_text&#10;                                break&#10;&#10;                        # 상세 페이지에서 요약 내용 가져오기&#10;                        summary = self._get_research_detail_summary(link)&#10;&#10;                        # 증권사/제공자 정보 찾기&#10;                        provider = &quot;&quot;&#10;                        for cell in cells:&#10;                            cell_text = cell.get_text(strip=True)&#10;                            if any(keyword in cell_text for keyword in ['증권', '투자', '자산', '캐피탈']):&#10;                                provider = cell_text&#10;                                break&#10;&#10;                        # 회사명 추출 (종목분석의 경우)&#10;                        company = &quot;&quot;&#10;                        if category_name == '종목분석' and len(cells) &gt; 0:&#10;                            company = cells[0].get_text(strip=True)&#10;&#10;                        report_data = {&#10;                            'title': title,&#10;                            'link': link,&#10;                            'summary': summary,&#10;                            'provider': provider,&#10;                            'company': company,&#10;                            'publish_date': publish_date if publish_date else 'unknown',&#10;                            'category_name': category_name,&#10;                            'category_key': category_name.lower(),&#10;                            'crawled_at': datetime.now().isoformat()&#10;                        }&#10;&#10;                        all_reports.append(report_data)&#10;                        count += 1&#10;&#10;                        logger.info(f&quot;{category_name}: '{title}' 리포트 수집 ({count}/{limit})&quot;)&#10;&#10;                        # 서버 부하 방지&#10;                        time.sleep(1)&#10;&#10;                    except Exception as e:&#10;                        logger.error(f&quot;{category_name} 리포트 항목 처리 중 오류: {e}&quot;)&#10;                        continue&#10;&#10;                logger.info(f&quot;{category_name}: {count}개 최신 리포트 수집 완료&quot;)&#10;&#10;                # 카테고리 간 딜레이&#10;                time.sleep(2)&#10;&#10;            except Exception as e:&#10;                logger.error(f&quot;{category_name} 카테고리 크롤링 중 오류: {e}&quot;)&#10;                continue&#10;&#10;        return all_reports&#10;&#10;    def _get_news_detail(self, link: str) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        뉴스 상세 내용 크롤링&#10;&#10;        Args:&#10;            link: 뉴스 링크&#10;&#10;        Returns:&#10;            Dict: 뉴스 내용(전체 본문), HTML 컨텐츠, 발행일 등 정보&#10;        &quot;&quot;&quot;&#10;        try:&#10;            response = self.session.get(link)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            # 네이버 뉴스 본문 추출 - 여러 선택자 시도&#10;            content = &quot;&quot;&#10;            full_html = &quot;&quot;&#10;&#10;            # 1. dic_area ID를 가진 요소 (네이버 뉴스 표준 형식)&#10;            content_div = soup.find('article', {'id': 'dic_area'})&#10;            if not content_div:&#10;                content_div = soup.find('div', {'id': 'dic_area'})&#10;&#10;            # 2. 다른 일반적인 선택자들 시도&#10;            if not content_div:&#10;                content_div = soup.find('div', {'class': 'article'})&#10;            if not content_div:&#10;                content_div = soup.find('div', {'id': 'articleBody'})&#10;            if not content_div:&#10;                content_div = soup.find('div', {'class': 'newsct_body'})&#10;            if not content_div:&#10;                content_div = soup.find('div', {'class': 'newsct_article'})&#10;&#10;            # 컨텐츠 추출&#10;            if content_div:&#10;                # 텍스트만 추출&#10;                content = content_div.get_text(strip=True)&#10;&#10;                # HTML 전체 내용 저장&#10;                full_html = str(content_div)&#10;&#10;            # 발행일 추출 (메타 태그 또는 기사 본문 내에서 추출 시도)&#10;            publish_date = &quot;&quot;&#10;            date_meta = soup.find('meta', {'property': 'article:published_time'})&#10;            if date_meta and date_meta.get('content'):&#10;                publish_date = date_meta['content']&#10;            else:&#10;                # 본문 내에서 날짜 형식 추출 (예: 2023.03.15. 10:30)&#10;                import re&#10;                date_patterns = [r'(\d{4}[.\-]\d{1,2}[.\-]\d{1,2})', r'(\d{1,2}[.\-]\d{1,2}[.\-]\d{2,4})']&#10;                for pattern in date_patterns:&#10;                    match = re.search(pattern, content)&#10;                    if match:&#10;                        publish_date = match.group(0)&#10;                        break&#10;&#10;            return {&#10;                'content': content,&#10;                'full_html': full_html,&#10;                'publish_date': publish_date&#10;            }&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;뉴스 상세 크롤링 중 오류: {e}&quot;)&#10;            return {}&#10;&#10;    def _get_research_detail_summary(self, link: str) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        리서치 리포트 상세 내용에서 요약 추출&#10;&#10;        Args:&#10;            link: 리포트 링크&#10;&#10;        Returns:&#10;            str: 리포트 요약 내용&#10;        &quot;&quot;&quot;&#10;        try:&#10;            response = self.session.get(link)&#10;            response.raise_for_status()&#10;&#10;            soup = BeautifulSoup(response.content, 'html.parser')&#10;&#10;            # 요약 내용 추출 (일반적으로 'div.summary' 또는 'div#reportSummary'에 요약이 있음)&#10;            summary_div = soup.find('div', {'class': 'summary'})&#10;            if not summary_div:&#10;                summary_div = soup.find('div', {'id': 'reportSummary'})&#10;&#10;            summary = summary_div.get_text(strip=True) if summary_div else &quot;&quot;&#10;&#10;            return summary&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;리포트 요약 크롤링 중 오류: {e}&quot;)&#10;            return &quot;&quot;&#10;&#10;    def get_today_summary(self) -&gt; Dict:&#10;        &quot;&quot;&quot;&#10;        오늘의 주요 뉴스와 리포트 요약 크롤링&#10;&#10;        Returns:&#10;            Dict: 오늘의 뉴스와 리포트 데이터&#10;        &quot;&quot;&quot;&#10;        logger.info(&quot;오늘의 주요 뉴스 크롤링 시작&quot;)&#10;&#10;        # 메인 뉴스 크롤링 (20개로 증가)&#10;        main_news = self.get_main_news(limit=20)&#10;        logger.info(f&quot;메인 뉴스 {len(main_news)}개 수집 완료&quot;)&#10;&#10;        # 리서치 리포트 크롤링 (복구)&#10;        research_reports = self.get_research_reports(limit=5)&#10;        logger.info(f&quot;리서치 리포트 {len(research_reports)}개 수집 완료&quot;)&#10;&#10;        return {&#10;            'main_news': main_news,&#10;            'research_reports': research_reports,&#10;            'crawled_at': datetime.now().isoformat(),&#10;            'total_count': len(main_news) + len(research_reports)&#10;        }&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # 테스트 실행&#10;    crawler = NaverStockNewsCrawler()&#10;    data = crawler.get_today_summary()&#10;    print(f&quot;총 {data['total_count']}개 뉴스/리포트 수집 완료&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>