"""
ë‰´ìŠ¤ ë° ë¦¬ì„œì¹˜ ë¶„ì„ ëª¨ë“ˆ - Gemini API í™œìš© (í†µí•© í¬ë¡¤ë§ ê¸°ëŠ¥ í¬í•¨)
"""

from typing import List, Dict
import json
import logging
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time
import re
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from google import genai
from google.genai import types

# ë¡œï¿½ï¿½ï¿½ ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

class IntegratedNewsAnalyzer:
    """í†µí•© ë‰´ìŠ¤ ë° ë¦¬ì„œì¹˜ í¬ë¡¤ë§ & ë¶„ì„ê¸°"""

    def __init__(self):
        self.model_name = "gemini-2.0-flash-001"
        self.client = genai.Client(api_key=os.getenv("GOOGLE_AI_API_KEY"))
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def ask_question_to_gemini_cache(self, prompt, max_retries=5, retry_delay=5):
        """
        Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì–»ìŠµë‹ˆë‹¤.
        ë‰´ìŠ¤ ë¶„ì„ì— ìµœì í™”ëœ ë²„ì „ì…ë‹ˆë‹¤.
        """
        start_time = time.time()

        for attempt in range(max_retries):
            try:
                # API í‚¤ í™•ì¸
                api_key = os.getenv("GOOGLE_AI_API_KEY")
                if not api_key:
                    raise Exception("GOOGLE_AI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                # Gemini API í˜¸ì¶œ
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.3,
                        max_output_tokens=2048
                    )
                )

                return response.text

            except Exception as e:
                error_msg = str(e).lower()
                print(f"API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")

                if hasattr(e, 'code') and e.code == 503:
                    print(f"â³ API ì‚¬ìš©ëŸ‰ í•œë„ ì´ˆê³¼ (ì‹œë„ {attempt + 1}/{max_retries}). {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                    continue

                if attempt == max_retries - 1:
                    return f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}"

                time.sleep(retry_delay)

        return "ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨"

    def json_match(self, text):
        """
        í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        """
        try:
            # ì¤‘ê´„í˜¸ íŒ¨í„´ ë§¤ì¹­
            pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.findall(pattern, text, re.DOTALL)

            for match in matches:
                try:
                    return json.loads(match)
                except json.JSONDecodeError:
                    continue

            # ë°±í‹±ìœ¼ë¡œ ê°ì‹¸ì§„ JSON ì°¾ê¸°
            json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            matches = re.findall(json_pattern, text, re.DOTALL)

            for match in matches:
                try:
                    return json.loads(match)
                except json.JSONDecodeError:
                    continue

            return None
        except Exception as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def create_news_analysis_prompt(self, news_text):
        """ë‰´ìŠ¤ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{news_text}

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì œê³µí•´ì£¼ì„¸ìš”:

{{
  "overall_sentiment": "positive/negative/neutral",
  "sentiment_score": 0-100,
  "key_themes": ["ì£¼ìš” í…Œë§ˆ1", "ì£¼ìš” í…Œë§ˆ2"],
  "market_impact": "ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„",
  "summary": "ì „ì²´ ë‰´ìŠ¤ ìš”ì•½",
  "investment_signals": "buy/sell/hold"
}}
"""

    def create_research_reports_analysis_prompt(self, reports_text):
        """ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{reports_text}

ë¶„ì„ ï¿½ï¿½ï¿½ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì œê³µí•´ì£¼ì„¸ìš”:

{{
  "category_summary": {{
    "ì¢…ëª©ë¶„ì„": "ì¢…ëª©ë¶„ì„ ìš”ì•½",
    "ì‚°ì—…ë¶„ì„": "ì‚°ì—…ë¶„ì„ ìš”ì•½", 
    "ì‹œí™©ì •ë³´": "ì‹œí™©ì •ë³´ ìš”ì•½",
    "íˆ¬ìì •ë³´": "íˆ¬ìì •ë³´ ìš”ì•½"
  }},
  "top_mentioned_stocks": ["ì¢…ëª©1", "ì¢…ëª©2", "ì¢…ëª©3"],
  "key_industries": ["ì—…ì¢…1", "ì—…ì¢…2", "ì—…ì¢…3"],
  "investment_themes": ["íˆ¬ìí…Œë§ˆ1", "íˆ¬ìí…Œë§ˆ2"],
  "market_outlook": "positive/negative/neutral",
  "risk_factors": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
  "opportunities": ["ê¸°íšŒ1", "ê¸°íšŒ2"],
  "analyst_consensus": "ì• ë„ë¦¬ìŠ¤íŠ¸ consensus",
  "summary": "ì „ì²´ ë¦¬í¬íŠ¸ ì¢…í•© ï¿½ï¿½ï¿½ì•½"
}}
"""

    def crawl_and_analyze_all(self, news_section_id: str = "101", news_limit: int = 20, reports_limit: int = 10) -> Dict:
        """
        ë‰´ìŠ¤ì™€ ë¦¬í¬íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  ë¶„ì„í•˜ì—¬ í†µí•©ëœ ê²°ê³¼ ë°˜í™˜

        Args:
            news_section_id: ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ ID (101: ì •ì¹˜, 102: ê²½ì œ, 103: ì‚¬íšŒ ë“±)
            news_limit: í¬ë¡¤ë§í•  ë‰´ìŠ¤ ê°œìˆ˜
            reports_limit: í¬ë¡¤ë§í•  ë¦¬í¬íŠ¸ ê°œìˆ˜ (ì¹´í…Œê³ ë¦¬ë³„)

        Returns:
            Dict: í†µí•©ï¿½ï¿½ í¬ë¡¤ë§ ë° ë¶„ì„ ê²°ê³¼
        """
        logger.info("=" * 60)
        logger.info("í†µí•© ë‰´ìŠ¤ & ë¦¬ì„œì¹˜ í¬ë¡¤ï¿½ï¿½ï¿½ ë° ë¶„ì„ ì‹œì‘")
        logger.info("=" * 60)

        # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
        logger.info("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ ì‹œì‘...")
        news_data = self._crawl_naver_news(news_section_id, news_limit)

        # 2. ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ í¬ë¡¤ë§
        logger.info("ğŸ“Š ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
        reports_data = self._crawl_research_reports(reports_limit)

        # 3. ë°ì´í„° ë¶„ì„
        logger.info("ğŸ¤– AI ê¸°ë°˜ ë°ì´í„° ë¶„ì„ ì‹œì‘...")

        # ë‰´ìŠ¤ ë¶„ì„
        news_analysis = {}
        if news_data:
            news_analysis = self.analyze_news_sentiment(news_data)

        # ë¦¬í¬íŠ¸ ë¶„ì„
        reports_analysis = {}
        if reports_data:
            reports_analysis = self.analyze_research_reports(reports_data)

        # 4. í†µí•© ê²°ê³¼ ìƒì„±
        integrated_result = {
            'metadata': {
                'crawled_at': datetime.now().isoformat(),
                'news_section_id': news_section_id,
                'news_count': len(news_data),
                'reports_count': len(reports_data),
                'source': 'integrated_news_research_crawler'
            },
            'news': {
                'data': news_data,
                'analysis': news_analysis
            },
            'research_reports': {
                'data': reports_data,
                'analysis': reports_analysis
            },
            'summary': {
                'total_items': len(news_data) + len(reports_data),
                'successful_news_crawl': len([n for n in news_data if n.get('content') and len(n['content']) > 50]),
                'successful_reports_crawl': len([r for r in reports_data if r.get('summary') and len(r['summary']) > 50]),
                'news_sentiment': news_analysis.get('overall_sentiment', 'unknown'),
                'reports_outlook': reports_analysis.get('overall_outlook', 'unknown')
            }
        }

        # 5. JSON íŒŒì¼ ì €ì¥
        logger.info("ğŸ’¾ í†µí•© ê²°ê³¼ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
        filename = self._save_integrated_json(integrated_result)

        if filename:
            logger.info(f"âœ… í†µí•© JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            integrated_result['saved_file'] = filename

        logger.info("=" * 60)
        logger.info("í†µí•© í¬ë¡¤ë§ ë° ë¶„ì„ ì™„ë£Œ")
        logger.info("=" * 60)

        return integrated_result

    def _crawl_naver_news(self, section_id: str, limit: int) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ (test_headline_crawler.py ê¸°ëŠ¥ í†µí•©)"""
        news_list = []

        try:
            url = f"https://news.naver.com/section/{section_id}"
            logger.info(f"ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ë§: {url}")

            response = self.session.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘
            headline_selectors = [
                '.section_headline .sa_text_title',
                '.section_latest .sa_text_title',
                '.sa_text_title',
                'a[href*="/article/"]',
            ]

            news_links = []
            for selector in headline_selectors:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        if element.name == 'a':
                            link = element
                        else:
                            link = element.find_parent('a') or element.find('a')

                        if link and link.get('href'):
                            href = link.get('href')
                            title = link.get_text(strip=True)

                            if '/article/' in href and title and len(title) > 5:
                                full_url = href if href.startswith('http') else f"https://news.naver.com{href}"
                                news_links.append({
                                    'title': title,
                                    'url': full_url
                                })

                        if len(news_links) >= limit:
                            break

                    if len(news_links) >= limit:
                        break

                except Exception as e:
                    logger.warning(f"ì„ íƒì '{selector}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            # ì¤‘ë³µ ì œê±°
            seen_urls = set()
            unique_news = []
            for news in news_links:
                if news['url'] not in seen_urls:
                    seen_urls.add(news['url'])
                    unique_news.append(news)
                    if len(unique_news) >= limit:
                        break

            logger.info(f"ğŸ“‹ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ {len(unique_news)}ê°œ ë°œê²¬")

            # ê° ë‰´ìŠ¤ì˜ ë³¸ë¬¸ í¬ë¡¤ë§
            for idx, news_item in enumerate(unique_news):
                try:
                    logger.info(f"ğŸ“– ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§... ({idx + 1}/{len(unique_news)})")

                    content_data = self._crawl_news_content(news_item['url'])

                    news_data = {
                        'id': idx + 1,
                        'title': news_item['title'],
                        'url': news_item['url'],
                        'content': content_data.get('content', ''),
                        'summary': content_data.get('summary', ''),
                        'publish_date': content_data.get('publish_date', ''),
                        'media': content_data.get('media', ''),
                        'category': f'section_{section_id}',
                        'crawled_at': datetime.now().isoformat()
                    }

                    news_list.append(news_data)
                    logger.info(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ ({len(news_list)}/{limit}): '{news_item['title'][:50]}...'")

                    time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

                except Exception as e:
                    logger.error(f"ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    # ê¸°ë³¸ ì •ë³´ë¼ë„ ì €ì¥
                    news_data = {
                        'id': idx + 1,
                        'title': news_item['title'],
                        'url': news_item['url'],
                        'content': '',
                        'summary': '',
                        'publish_date': '',
                        'media': '',
                        'category': f'section_{section_id}',
                        'crawled_at': datetime.now().isoformat()
                    }
                    news_list.append(news_data)
                    continue

            logger.info(f"ğŸ‰ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(news_list)}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

        return news_list

    def _crawl_news_content(self, url: str) -> Dict:
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ (art_crawl ë°©ì‹)"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ì •í™•í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ì„ íƒì ì‚¬ìš©
            title_selector = "#title_area > span"
            date_selector = "#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div:nth-child(1) > span"
            main_selector = "#dic_area"

            # ì œëª© ì¶”ì¶œ
            title = ''
            try:
                title_elements = soup.select(title_selector)
                title_lst = [t.get_text(strip=True) for t in title_elements]
                title = "".join(title_lst)
            except:
                pass

            # ë°œí–‰ì¼ ì¶”ì¶œ
            publish_date = ''
            try:
                date_elements = soup.select(date_selector)
                date_lst = [d.get_text(strip=True) for d in date_elements]
                publish_date = "".join(date_lst)
            except:
                # ëŒ€ì²´ ì„ íƒìë“¤
                fallback_selectors = [
                    '.media_end_head_info_datestamp_time',
                    'span[data-date-time]',
                    '.author em'
                ]
                for selector in fallback_selectors:
                    try:
                        date_element = soup.select_one(selector)
                        if date_element:
                            publish_date = date_element.get_text(strip=True)
                            break
                    except:
                        continue

            # ë³¸ë¬¸ ì¶”ì¶œ
            content = ''
            try:
                main_elements = soup.select(main_selector)
                main_lst = []
                for m in main_elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for script in m(["script", "style", "iframe"]):
                        script.decompose()

                    # ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹í™” ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
                    unwanted_selectors = [
                        '.end_photo_org',
                        '.media_end_head_journalist',
                        '.media_end_summary',
                        'strong.media_end_summary'
                    ]
                    for unwanted_selector in unwanted_selectors:
                        for unwanted in m.select(unwanted_selector):
                            unwanted.decompose()

                    m_text = m.get_text(strip=True)
                    if m_text:
                        main_lst.append(m_text)

                content = " ".join(main_lst)

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()

                # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
                unwanted_phrases = [
                    "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€",
                    "ì €ì‘ê¶Œì",
                    "â“’",
                    "Copyright"
                ]
                for phrase in unwanted_phrases:
                    if phrase in content:
                        content = content.split(phrase)[0].strip()

            except:
                # ï¿½ï¿½ï¿½ì²´ ì„ íƒìë“¤
                fallback_selectors = [
                    '#articleBodyContents',
                    '.go_trans._article_content',
                    '._article_body_contents'
                ]
                for selector in fallback_selectors:
                    try:
                        content_element = soup.select_one(selector)
                        if content_element:
                            for script in content_element(["script", "style"]):
                                script.decompose()
                            content = content_element.get_text(strip=True)
                            if len(content) > 50:
                                break
                    except:
                        continue

            # ìš”ì•½ ìƒì„±
            summary = ''
            if content:
                sentences = content.split('.')
                if sentences:
                    summary = sentences[0][:200] + ('...' if len(sentences[0]) > 200 else '')

            # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
            media = ''
            media_selectors = [
                '.media_end_head_top_logo img',
                '.press_logo img',
                '.media_end_head_top_logo_text'
            ]
            for selector in media_selectors:
                try:
                    media_element = soup.select_one(selector)
                    if media_element:
                        if media_element.name == 'img':
                            media = media_element.get('alt', '')
                        else:
                            media = media_element.get_text(strip=True)
                        if media:
                            break
                except:
                    continue

            return {
                'content': content,
                'summary': summary,
                'publish_date': publish_date,
                'media': media,
                'title': title
            }

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return {
                'content': '',
                'summary': '',
                'publish_date': '',
                'media': '',
                'title': ''
            }

    def _crawl_research_reports(self, limit: int) -> List[Dict]:
        """ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ í¬ë¡¤ë§"""
        all_reports = []

        # ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ ì¹´í…Œê³ ë¦¬ë³„ URL
        category_urls = {
            'ì¢…ëª©ë¶„ì„': "https://finance.naver.com/research/company_list.naver",
            'ì‚°ì—…ë¶„ì„': "https://finance.naver.com/research/industry_list.naver",
            'ì‹œí™©ì •ë³´': "https://finance.naver.com/research/market_info_list.naver",
            'íˆ¬ìì •ë³´': "https://finance.naver.com/research/invest_list.naver"
        }

        logger.info(f"ğŸ“ˆ ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ì—ì„œ ê° ì¹´í…Œï¿½ï¿½ï¿½ë¦¬ë³„ ìµœì‹  ë¦¬í¬íŠ¸ {limit}ê°œì”© ìˆ˜ì§‘")

        for category_name, category_url in category_urls.items():
            try:
                logger.info(f"ğŸ“Š {category_name} ë¦¬í¬íŠ¸ í¬ë¡¤ë§... ({category_url})")

                response = self.session.get(category_url, timeout=10)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')

                # ë¦¬í¬íŠ¸ í…Œì´ë¸” ì°¾ê¸°
                table_selectors = [
                    'table.type_1',
                    'table.type_2',
                    'table[summary*="ë¦¬í¬íŠ¸"]',
                    'div.box_type_m table',
                    'table'
                ]

                table = None
                for selector in table_selectors:
                    table = soup.select_one(selector)
                    if table and len(table.find_all('tr')) > 1:
                        logger.info(f"{category_name} ë¦¬í¬íŠ¸ í…Œì´ë¸” ë°œê²¬")
                        break

                if not table:
                    logger.warning(f"{category_name}: ë¦¬í¬íŠ¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                rows = table.find_all('tr')
                data_rows = rows[1:] if rows[0].find('th') else rows

                count = 0
                for row in data_rows:
                    if count >= limit:
                        break

                    cells = row.find_all('td')
                    if len(cells) < 2:
                        continue

                    try:
                        # ì œëª©ê³¼ ë§í¬ ì°¾ê¸°
                        title = ""
                        link = ""

                        for cell in cells[:3]:
                            a_tag = cell.find('a')
                            if a_tag and a_tag.get('href'):
                                title = a_tag.get_text(strip=True)
                                link = a_tag.get('href')

                                if len(title) > 5 and not title.isdigit():
                                    break

                        if not title or not link:
                            continue

                        # URL ì •ë¦¬
                        if link.startswith('/'):
                            link = "https://finance.naver.com" + link
                        elif not link.startswith('http'):
                            link = f"https://finance.naver.com/research/{link}"

                        # ë°œí–‰ì¼ ì¶”ì¶œ
                        publish_date = ""
                        for cell in reversed(cells):
                            cell_text = cell.get_text(strip=True)
                            if self._is_valid_date(cell_text):
                                publish_date = cell_text
                                break

                        # ì¦ê¶Œì‚¬ ì •ë³´ ì¶”ì¶œ
                        provider = ""
                        for cell in cells:
                            cell_text = cell.get_text(strip=True)
                            if any(keyword in cell_text for keyword in ['ì¦ê¶Œ', 'íˆ¬ì', 'ìì‚°', 'ìºí”¼íƒˆ', 'Securities']):
                                provider = cell_text
                                break

                        # ë¦¬í¬íŠ¸ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ê°„ì†Œí™”)
                        content = self._crawl_report_content(link)

                        report_data = {
                            'id': len(all_reports) + 1,
                            'title': title,
                            'link': link,
                            'summary': content if content else title,  # ë³¸ë¬¸ì„ summaryë¡œ ì €ì¥
                            'provider': provider,
                            'publish_date': publish_date if publish_date else 'unknown',
                            'category_name': category_name,
                            'category_key': category_name.lower(),
                            'crawled_at': datetime.now().isoformat()
                        }

                        all_reports.append(report_data)
                        count += 1

                        logger.info(f"âœ… {category_name}: '{title[:30]}...' ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ ({count}/{limit})")

                        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

                    except Exception as e:
                        logger.error(f"{category_name} ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                logger.info(f"ğŸ¯ {category_name}: {count}ê°œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
                time.sleep(2)  # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´

            except Exception as e:
                logger.error(f"{category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        logger.info(f"ğŸ“‹ ì „ì²´ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_reports)}ê°œ ìˆ˜ì§‘")
        return all_reports

    def _crawl_report_content(self, link: str) -> str:
        """ë¦¬í¬íŠ¸ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ê°„ì†Œí™”)"""
        try:
            response = self.session.get(link, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            content_selectors = [
                'div.view_cnt',
                'td.view_cnt',
                'div.report_content',
                'div.content',
                'div[class*="content"]'
            ]

            for selector in content_selectors:
                try:
                    content_div = soup.select_one(selector)
                    if content_div:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_div.select('script, style, iframe, .ad, .advertisement'):
                            unwanted.decompose()

                        text = content_div.get_text(separator=' ', strip=True)

                        # í…ìŠ¤íŠ¸ ì •ë¦¬
                        text = re.sub(r'\s+', ' ', text).strip()

                        if len(text) > 100:
                            return text[:500] + "..." if len(text) > 500 else text
                except:
                    continue

            return ""

        except Exception as e:
            logger.warning(f"ë¦¬í¬íŠ¸ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({link}): {e}")
            return ""

    def _is_valid_date(self, text: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ ê²€ì¦"""
        import re
        date_patterns = [
            r'\d{4}[-./]\d{1,2}[-./]\d{1,2}',
            r'\d{1,2}[-./]\d{1,2}[-./]\d{2,4}',
            r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼',
            r'\d{2}\.\d{2}\.\d{2}',
            r'\d{4}\.\d{2}\.\d{2}',
        ]

        for pattern in date_patterns:
            if re.search(pattern, text):
                return True
        return False

    def _save_integrated_json(self, data: Dict, filename: str = None) -> str:
        """í†µí•© ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"integrated_news_research_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“ í†µí•© JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename

        except Exception as e:
            logger.error(f"JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def analyze_news_sentiment(self, news_data: List[Dict]) -> Dict:
        """
        ë‰´ìŠ¤ ê°ì • ë¶„ì„

        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            Dict: ê°ì • ë¶„ì„ ê²°ê³¼
        """
        if not news_data:
            return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ë‰´ìŠ¤ ì œëª©ê³¼ ë‚´ìš©ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        combined_text = ""
        for idx, news in enumerate(news_data, 1):
            combined_text += f"\n\n--- ë‰´ìŠ¤ {idx} ---\n"
            combined_text += f"ì œëª©: {news.get('title', '')}\n"
            # contentê°€ ë¹„ì–´ìˆìœ¼ë©´ ì œëª©ë§Œ ì‚¬ìš©
            content = news.get('content', '')
            if content.strip():
                combined_text += f"ë‚´ìš©: {content[:500]}...\n"
            else:
                combined_text += f"ë‚´ìš©: ì œëª© ì°¸ì¡°\n"

        # í†µí•©ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©
        prompt = self.create_news_analysis_prompt(combined_text)

        try:
            response = self.ask_question_to_gemini_cache(prompt)

            parsed_result = self.json_match(response)

            if parsed_result:
                parsed_result['analyzed_at'] = datetime.now().isoformat()
                parsed_result['news_count'] = len(news_data)
                return parsed_result
            else:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "overall_sentiment": "neutral",
                    "sentiment_score": 0,
                    "key_themes": ["ë¶„ì„ ì‹¤íŒ¨"],
                    "market_impact": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ìƒì„¸ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "summary": "ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "investment_signals": "hold",
                    "analyzed_at": datetime.now().isoformat(),
                    "news_count": len(news_data),
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def analyze_research_reports(self, reports_data: List[Dict]) -> Dict:
        """
        ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„ (ê°œì„ ëœ ë²„ì „ - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ í¬ê´„)

        Args:
            reports_data: ë¦¬í¬íŠ¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            Dict: ë¦¬í¬íŠ¸ ë¶„ì„ ê²°ê³¼
        """
        if not reports_data:
            return {"error": "ë¶„ì„í•  ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¦¬í¬íŠ¸ ë¶„ë¥˜
        categorized_reports = {}
        for report in reports_data:
            category = report.get('category_name', 'unknown')
            if category not in categorized_reports:
                categorized_reports[category] = []
            categorized_reports[category].append(report)

        # ë¶„ì„ìš© í…ìŠ¤íŠ¸ ìƒì„±
        combined_text = self._format_reports_for_analysis(categorized_reports)

        # í†µí•©ëœ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = self.create_research_reports_analysis_prompt(combined_text)

        try:
            response = self.ask_question_to_gemini_cache(prompt)

            parsed_result = self.json_match(response)

            if parsed_result:
                # í•„ìˆ˜ í•­ëª©ì´ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ì§€ ê²€ì¦
                required_fields = [
                    'category_summary', 'top_mentioned_stocks', 'key_industries',
                    'investment_themes', 'market_outlook', 'risk_factors',
                    'opportunities', 'analyst_consensus', 'summary'
                ]

                # ëˆ„ë½ëœ í•„ë“œê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                for field in required_fields:
                    if field not in parsed_result:
                        if field == 'category_summary':
                            parsed_result[field] = {"ì¢…ëª©ë¶„ì„": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±", "ì‚°ì—…ë¶„ì„": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±", "ì‹œí™©ì •ë³´": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±", "íˆ¬ìì •ë³´": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±"}
                        elif field in ['top_mentioned_stocks', 'key_industries', 'investment_themes', 'risk_factors', 'opportunities']:
                            parsed_result[field] = ["ë°ì´í„° ë¶€ì¡±"]
                        elif field == 'market_outlook':
                            parsed_result[field] = "neutral"
                        elif field in ['analyst_consensus', 'summary']:
                            parsed_result[field] = "ë¶„ì„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

                parsed_result['analyzed_at'] = datetime.now().isoformat()
                parsed_result['reports_count'] = len(reports_data)
                parsed_result['category_counts'] = {
                    cat: len(reports) for cat, reports in categorized_reports.items()
                }
                return parsed_result
            else:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "category_summary": {
                        "ì¢…ëª©ë¶„ì„": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "ì‚°ì—…ë¶„ì„": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "ì‹œí™©ì •ë³´": "JSON íŒŒì‹± ï¿½ï¿½ï¿½íŒ¨ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "íˆ¬ìì •ë³´": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    },
                    "top_mentioned_stocks": ["ë¶„ì„ ì‹¤íŒ¨"],
                    "key_industries": ["ë¶„ì„ ì‹¤íŒ¨"],
                    "investment_themes": ["ë¶„ì„ ì‹¤íŒ¨"],
                    "market_outlook": "neutral",
                    "risk_factors": ["JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ë¦¬ìŠ¤í¬ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                    "opportunities": ["JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°íšŒ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                    "analyst_consensus": "ë¦¬í¬íŠ¸ ë¶„ì„ ì¤‘ JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "summary": "ì „ì²´ ë¦¬í¬íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìƒì„¸ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "raw_response": response,
                    "analyzed_at": datetime.now().isoformat(),
                    "reports_count": len(reports_data),
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }

        except Exception as e:
            logger.error(f"ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ï¿½ï¿½ï¿½ ë°œìƒ: {str(e)}"}

    def _format_reports_for_analysis(self, categorized_reports: Dict) -> str:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•  í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        """
        formatted_text = ""

        category_names = {
            'stock_analysis': 'ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸',
            'industry_analysis': 'ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸',
            'market_info': 'ì‹œí™©ì •ë³´ ë¦¬í¬íŠ¸',
            'investment_info': 'íˆ¬ìì •ë³´ ë¦¬í¬íŠ¸',
            'ì¢…ëª©ë¶„ì„': 'ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸',
            'ì‚°ì—…ë¶„ì„': 'ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸',
            'ì‹œí™©ì •ë³´': 'ì‹œí™©ì •ë³´ ë¦¬í¬íŠ¸',
            'íˆ¬ìì •ë³´': 'íˆ¬ìì •ë³´ ë¦¬í¬íŠ¸'
        }

        for category, reports in categorized_reports.items():
            if reports:
                display_name = category_names.get(category, category)
                formatted_text += f"\n\n=== {display_name} ===\n"

                for idx, report in enumerate(reports, 1):
                    formatted_text += f"\n{idx}. ì œëª©: {report.get('title', 'N/A')}\n"

                    if report.get('provider'):
                        formatted_text += f"   ì¦ê¶Œì‚¬: {report['provider']}\n"

                    # summaryê°€ ë¹„ì–´ìˆìœ¼ë©´ ì œëª©ìœ¼ë¡œ ëŒ€ì²´
                    summary = report.get('summary', '')
                    if summary.strip() and summary != "ìš”ì•½ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        formatted_text += f"   ìš”ì•½: {summary[:200]}...\n"
                    else:
                        formatted_text += f"   ìš”ì•½: ì œëª© ì°¸ì¡°\n"

                    if report.get('publish_date'):
                        formatted_text += f"   ë‚ ì§œ: {report['publish_date']}\n"

        return formatted_text

    def analyze_comprehensive_with_categories(self, crawled_data: Dict) -> Dict:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„ì„ í¬í•¨í•œ ì¢…í•© ë¶„ì„

        Args:
            crawled_data: í¬ë¡¤ë§ëœ ì „ì²´ ë°ì´í„°

        Returns:
            Dict: ì¢…í•© ë¶„ì„ ê²°ê³¼ (ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ ë¶„ì„ í¬í•¨)
        """
        logger.info("ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì¢…í•© ë¶„ì„ ì‹œì‘")

        # ì „ì²´ ë‰´ìŠ¤ ê°ì • ë¶„ì„
        news_analysis = self.analyze_news_sentiment(crawled_data.get('main_news', []))
        logger.info("ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì™„ë£Œ")

        # ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„
        reports_analysis = self.analyze_research_reports(crawled_data.get('research_reports', []))
        logger.info("ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ë¶„ì„ ì™„ë£Œ")

        # ì¹´í…Œê³ ë¦¬ë³„ ì‹¬í™” ë¶„ì„
        category_insights = self._generate_category_insights(crawled_data.get('research_reports', []))
        logger.info("ì¹´í…Œê³ ë¦¬ë³„ ì‹¬í™” ë¶„ì„ ì™„ë£Œ")

        # ì¼ì¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        daily_report = self.generate_enhanced_daily_report(news_analysis, reports_analysis, category_insights)
        logger.info("ì¼ì¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

        return {
            'news_analysis': news_analysis,
            'reports_analysis': reports_analysis,
            'category_insights': category_insights,
            'daily_report': daily_report,
            'meta': {
                'total_analyzed': len(crawled_data.get('main_news', [])) + len(crawled_data.get('research_reports', [])),
                'analysis_completed_at': datetime.now().isoformat()
            }
        }

    def _generate_category_insights(self, reports_data: List[Dict]) -> Dict:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì‹¬í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        """
        if not reports_data:
            return {"error": "ë¶„ì„í•  ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for report in reports_data:
            category = report.get('category_name', 'Unknown')
            if category not in category_stats:
                category_stats[category] = {
                    'count': 0,
                    'firms': set(),
                    'stocks': set(),
                    'recent_titles': []
                }

            category_stats[category]['count'] += 1

            if report.get('provider'):
                category_stats[category]['firms'].add(report['provider'])

            if report.get('title'):
                category_stats[category]['recent_titles'].append(report['title'])

        # í†µê³„ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ï¿½ï¿½ï¿½íƒœë¡œ ë³€í™˜
        formatted_stats = {}
        for category, stats in category_stats.items():
            formatted_stats[category] = {
                'count': stats['count'],
                'active_firms': list(stats['firms'])[:5],  # ìµœëŒ€ 5ê°œ
                'mentioned_stocks': list(stats['stocks'])[:10],  # ìµœëŒ€ 10ê°œ
                'sample_titles': stats['recent_titles'][:3]  # ìµœëŒ€ 3ê°œ
            }

        return {
            'category_statistics': formatted_stats,
            'total_categories': len(category_stats),
            'most_active_category': max(category_stats.keys(), key=lambda k: category_stats[k]['count']) if category_stats else None,
            'generated_at': datetime.now().isoformat()
        }

    def generate_enhanced_daily_report(self, news_analysis: Dict, reports_analysis: Dict, category_insights: Dict) -> Dict:
        """
        ì¹´í…Œê³ ë¦¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ê°œì„ ëœ ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±
        """
        # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
        try:
            sentiment_score = news_analysis.get('sentiment_score', 50)
            market_sentiment_score = max(1, min(10, int(sentiment_score / 10)))

            return {
                'market_sentiment_score': market_sentiment_score,
                'confidence_level': 7,  # ê¸°ë³¸ ì‹ ë¢°ë„
                'summary': f"ë‰´ìŠ¤ {news_analysis.get('news_count', 0)}ê°œ, ë¦¬í¬íŠ¸ {reports_analysis.get('reports_count', 0)}ê°œ ë¶„ì„ ì™„ë£Œ",
                'recommendations': [
                    "ì‹œì¥ ë™í–¥ì„ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”",
                    "ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ì² ì €íˆ í•˜ì„¸ìš”"
                ],
                'generated_at': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"}
